{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Kaggle_TPS_202109_Optuna_LGBM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OYCEgYQNB8L"
      },
      "source": [
        "# Kaggle TPS 202109\n",
        "\n",
        "## Optuna LightGBM Starter\n",
        "\n",
        "### Check GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uGK9FLAGLO-a",
        "outputId": "69b45e80-f2d9-4f4c-f84b-c54fb8c2813b"
      },
      "source": [
        "# Google Colab\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Sep 18 04:24:55 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.63.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   40C    P0    26W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2ToaUCWOQKX"
      },
      "source": [
        "### Check Memory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3arbBbThOEtU",
        "outputId": "24b8c140-1309-4771-fd08-c36928b2dfaf"
      },
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your runtime has 54.8 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCrV8z4DOaIa"
      },
      "source": [
        "### PIP Installs\n",
        "\n",
        "Uninstall the colab default lightgbm, since it is not set up for GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YgvqMGdfqJds",
        "outputId": "6062e3e4-6916-443c-947a-27846fc4c9ac"
      },
      "source": [
        "!pip uninstall lightgbm\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: lightgbm 2.2.3\n",
            "Uninstalling lightgbm-2.2.3:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.7/dist-packages/lightgbm-2.2.3.dist-info/*\n",
            "    /usr/local/lib/python3.7/dist-packages/lightgbm/*\n",
            "Proceed (y/n)? y\n",
            "  Successfully uninstalled lightgbm-2.2.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nCdxlw1qj4M"
      },
      "source": [
        "For now, git clone the lightgbm from the repo.  (There is possibly another GPU friendly install option, but it hasn't been verified to work)\n",
        "\n",
        "Note that this will take up space, so this is being done before mounting Google Drive\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NPeEFasOqckw",
        "outputId": "8389ab8b-11da-45eb-aed4-0975f004e7d8"
      },
      "source": [
        "! git clone --recursive https://github.com/Microsoft/LightGBM\n",
        "! cd LightGBM && rm -rf build && mkdir build && cd build && cmake -DUSE_GPU=1 ../../LightGBM && make -j4 && cd ../python-package && python3 setup.py install --precompile --gpu;    \n",
        "# unfortunately I forgot where I got the above, so cannot give them adequate credit"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'LightGBM'...\n",
            "remote: Enumerating objects: 23494, done.\u001b[K\n",
            "remote: Counting objects: 100% (1171/1171), done.\u001b[K\n",
            "remote: Compressing objects: 100% (631/631), done.\u001b[K\n",
            "remote: Total 23494 (delta 731), reused 847 (delta 519), pack-reused 22323\u001b[K\n",
            "Receiving objects: 100% (23494/23494), 18.12 MiB | 23.97 MiB/s, done.\n",
            "Resolving deltas: 100% (17124/17124), done.\n",
            "Submodule 'include/boost/compute' (https://github.com/boostorg/compute) registered for path 'external_libs/compute'\n",
            "Submodule 'eigen' (https://gitlab.com/libeigen/eigen.git) registered for path 'external_libs/eigen'\n",
            "Submodule 'external_libs/fast_double_parser' (https://github.com/lemire/fast_double_parser.git) registered for path 'external_libs/fast_double_parser'\n",
            "Submodule 'external_libs/fmt' (https://github.com/fmtlib/fmt.git) registered for path 'external_libs/fmt'\n",
            "Cloning into '/content/LightGBM/external_libs/compute'...\n",
            "remote: Enumerating objects: 21731, done.        \n",
            "remote: Counting objects: 100% (3/3), done.        \n",
            "remote: Compressing objects: 100% (3/3), done.        \n",
            "remote: Total 21731 (delta 0), reused 1 (delta 0), pack-reused 21728        \n",
            "Receiving objects: 100% (21731/21731), 8.51 MiB | 25.78 MiB/s, done.\n",
            "Resolving deltas: 100% (17566/17566), done.\n",
            "Cloning into '/content/LightGBM/external_libs/eigen'...\n",
            "remote: Enumerating objects: 112177, done.        \n",
            "remote: Counting objects: 100% (1800/1800), done.        \n",
            "remote: Compressing objects: 100% (809/809), done.        \n",
            "remote: Total 112177 (delta 978), reused 1799 (delta 977), pack-reused 110377        \n",
            "Receiving objects: 100% (112177/112177), 103.53 MiB | 12.89 MiB/s, done.\n",
            "Resolving deltas: 100% (91706/91706), done.\n",
            "Cloning into '/content/LightGBM/external_libs/fast_double_parser'...\n",
            "remote: Enumerating objects: 689, done.        \n",
            "remote: Counting objects: 100% (189/189), done.        \n",
            "remote: Compressing objects: 100% (121/121), done.        \n",
            "remote: Total 689 (delta 93), reused 99 (delta 41), pack-reused 500        \n",
            "Receiving objects: 100% (689/689), 802.19 KiB | 7.43 MiB/s, done.\n",
            "Resolving deltas: 100% (347/347), done.\n",
            "Cloning into '/content/LightGBM/external_libs/fmt'...\n",
            "remote: Enumerating objects: 27426, done.        \n",
            "remote: Counting objects: 100% (168/168), done.        \n",
            "remote: Compressing objects: 100% (83/83), done.        \n",
            "remote: Total 27426 (delta 92), reused 122 (delta 59), pack-reused 27258        \n",
            "Receiving objects: 100% (27426/27426), 13.42 MiB | 22.16 MiB/s, done.\n",
            "Resolving deltas: 100% (18529/18529), done.\n",
            "Submodule path 'external_libs/compute': checked out '36c89134d4013b2e5e45bc55656a18bd6141995a'\n",
            "Submodule path 'external_libs/eigen': checked out '8ba1b0f41a7950dc3e1d4ed75859e36c73311235'\n",
            "Submodule path 'external_libs/fast_double_parser': checked out 'ace60646c02dc54c57f19d644e49a61e7e7758ec'\n",
            "Submodule 'benchmark/dependencies/abseil-cpp' (https://github.com/abseil/abseil-cpp.git) registered for path 'external_libs/fast_double_parser/benchmarks/dependencies/abseil-cpp'\n",
            "Submodule 'benchmark/dependencies/double-conversion' (https://github.com/google/double-conversion.git) registered for path 'external_libs/fast_double_parser/benchmarks/dependencies/double-conversion'\n",
            "Cloning into '/content/LightGBM/external_libs/fast_double_parser/benchmarks/dependencies/abseil-cpp'...\n",
            "remote: Enumerating objects: 15207, done.        \n",
            "remote: Counting objects: 100% (1142/1142), done.        \n",
            "remote: Compressing objects: 100% (598/598), done.        \n",
            "remote: Total 15207 (delta 753), reused 866 (delta 543), pack-reused 14065        \n",
            "Receiving objects: 100% (15207/15207), 10.30 MiB | 10.74 MiB/s, done.\n",
            "Resolving deltas: 100% (11494/11494), done.\n",
            "Cloning into '/content/LightGBM/external_libs/fast_double_parser/benchmarks/dependencies/double-conversion'...\n",
            "remote: Enumerating objects: 1235, done.        \n",
            "remote: Counting objects: 100% (79/79), done.        \n",
            "remote: Compressing objects: 100% (61/61), done.        \n",
            "remote: Total 1235 (delta 45), reused 38 (delta 18), pack-reused 1156        \n",
            "Receiving objects: 100% (1235/1235), 7.09 MiB | 17.25 MiB/s, done.\n",
            "Resolving deltas: 100% (817/817), done.\n",
            "Submodule path 'external_libs/fast_double_parser/benchmarks/dependencies/abseil-cpp': checked out 'd936052d32a5b7ca08b0199a6724724aea432309'\n",
            "Submodule path 'external_libs/fast_double_parser/benchmarks/dependencies/double-conversion': checked out 'f4cb2384efa55dee0e6652f8674b05763441ab09'\n",
            "Submodule path 'external_libs/fmt': checked out 'cc09f1a6798c085c325569ef466bcdcffdc266d4'\n",
            "-- The C compiler identification is GNU 7.5.0\n",
            "-- The CXX compiler identification is GNU 7.5.0\n",
            "-- Check for working C compiler: /usr/bin/cc\n",
            "-- Check for working C compiler: /usr/bin/cc -- works\n",
            "-- Detecting C compiler ABI info\n",
            "-- Detecting C compiler ABI info - done\n",
            "-- Detecting C compile features\n",
            "-- Detecting C compile features - done\n",
            "-- Check for working CXX compiler: /usr/bin/c++\n",
            "-- Check for working CXX compiler: /usr/bin/c++ -- works\n",
            "-- Detecting CXX compiler ABI info\n",
            "-- Detecting CXX compiler ABI info - done\n",
            "-- Detecting CXX compile features\n",
            "-- Detecting CXX compile features - done\n",
            "-- Found OpenMP_C: -fopenmp (found version \"4.5\") \n",
            "-- Found OpenMP_CXX: -fopenmp (found version \"4.5\") \n",
            "-- Found OpenMP: TRUE (found version \"4.5\")  \n",
            "-- Looking for CL_VERSION_2_2\n",
            "-- Looking for CL_VERSION_2_2 - found\n",
            "-- Found OpenCL: /usr/lib/x86_64-linux-gnu/libOpenCL.so (found version \"2.2\") \n",
            "-- OpenCL include directory: /usr/include\n",
            "-- Boost version: 1.65.1\n",
            "-- Found the following Boost libraries:\n",
            "--   filesystem\n",
            "--   system\n",
            "-- Performing Test MM_PREFETCH\n",
            "-- Performing Test MM_PREFETCH - Success\n",
            "-- Using _mm_prefetch\n",
            "-- Performing Test MM_MALLOC\n",
            "-- Performing Test MM_MALLOC - Success\n",
            "-- Using _mm_malloc\n",
            "-- Configuring done\n",
            "-- Generating done\n",
            "-- Build files have been written to: /content/LightGBM/build\n",
            "\u001b[35m\u001b[1mScanning dependencies of target _lightgbm\u001b[0m\n",
            "\u001b[35m\u001b[1mScanning dependencies of target lightgbm\u001b[0m\n",
            "[  1%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/main.cpp.o\u001b[0m\n",
            "[  2%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/boosting/boosting.cpp.o\u001b[0m\n",
            "[  4%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/application/application.cpp.o\u001b[0m\n",
            "[  5%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/boosting/boosting.cpp.o\u001b[0m\n",
            "[  7%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/boosting/gbdt.cpp.o\u001b[0m\n",
            "[  8%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/boosting/gbdt.cpp.o\u001b[0m\n",
            "[ 10%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/boosting/gbdt_model_text.cpp.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/boosting/gbdt_model_text.cpp.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/boosting/gbdt_prediction.cpp.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/boosting/gbdt_prediction.cpp.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/boosting/prediction_early_stop.cpp.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/io/bin.cpp.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/boosting/prediction_early_stop.cpp.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/io/config.cpp.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/io/config_auto.cpp.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/io/bin.cpp.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/io/dataset.cpp.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/io/config.cpp.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/io/dataset_loader.cpp.o\u001b[0m\n",
            "[ 28%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/io/file_io.cpp.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/io/config_auto.cpp.o\u001b[0m\n",
            "[ 31%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/io/dataset.cpp.o\u001b[0m\n",
            "[ 33%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/io/dataset_loader.cpp.o\u001b[0m\n",
            "[ 34%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/io/json11.cpp.o\u001b[0m\n",
            "[ 36%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/io/file_io.cpp.o\u001b[0m\n",
            "[ 37%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/io/json11.cpp.o\u001b[0m\n",
            "[ 39%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/io/metadata.cpp.o\u001b[0m\n",
            "[ 40%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/io/parser.cpp.o\u001b[0m\n",
            "[ 42%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/io/train_share_states.cpp.o\u001b[0m\n",
            "[ 43%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/io/metadata.cpp.o\u001b[0m\n",
            "[ 44%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/io/tree.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/io/parser.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/io/train_share_states.cpp.o\u001b[0m\n",
            "[ 49%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/io/tree.cpp.o\u001b[0m\n",
            "[ 50%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/metric/dcg_calculator.cpp.o\u001b[0m\n",
            "[ 52%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/metric/metric.cpp.o\u001b[0m\n",
            "[ 53%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/network/ifaddrs_patch.cpp.o\u001b[0m\n",
            "[ 55%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/network/linker_topo.cpp.o\u001b[0m\n",
            "[ 56%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/network/linkers_mpi.cpp.o\u001b[0m\n",
            "[ 57%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/network/linkers_socket.cpp.o\u001b[0m\n",
            "[ 59%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/network/network.cpp.o\u001b[0m\n",
            "[ 60%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/objective/objective_function.cpp.o\u001b[0m\n",
            "[ 62%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/metric/dcg_calculator.cpp.o\u001b[0m\n",
            "[ 63%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/treelearner/cuda_tree_learner.cpp.o\u001b[0m\n",
            "[ 65%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/metric/metric.cpp.o\u001b[0m\n",
            "[ 66%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/network/ifaddrs_patch.cpp.o\u001b[0m\n",
            "[ 68%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/treelearner/data_parallel_tree_learner.cpp.o\u001b[0m\n",
            "[ 69%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/network/linker_topo.cpp.o\u001b[0m\n",
            "[ 71%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/network/linkers_mpi.cpp.o\u001b[0m\n",
            "[ 72%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/network/linkers_socket.cpp.o\u001b[0m\n",
            "[ 73%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/network/network.cpp.o\u001b[0m\n",
            "[ 75%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/treelearner/feature_parallel_tree_learner.cpp.o\u001b[0m\n",
            "[ 76%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/objective/objective_function.cpp.o\u001b[0m\n",
            "[ 78%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/treelearner/cuda_tree_learner.cpp.o\u001b[0m\n",
            "[ 79%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/treelearner/gpu_tree_learner.cpp.o\u001b[0m\n",
            "[ 81%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/treelearner/linear_tree_learner.cpp.o\u001b[0m\n",
            "[ 82%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/treelearner/data_parallel_tree_learner.cpp.o\u001b[0m\n",
            "[ 84%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/treelearner/serial_tree_learner.cpp.o\u001b[0m\n",
            "[ 85%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/treelearner/tree_learner.cpp.o\u001b[0m\n",
            "[ 86%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/treelearner/voting_parallel_tree_learner.cpp.o\u001b[0m\n",
            "[ 88%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/treelearner/feature_parallel_tree_learner.cpp.o\u001b[0m\n",
            "[ 89%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/c_api.cpp.o\u001b[0m\n",
            "[ 91%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/treelearner/gpu_tree_learner.cpp.o\u001b[0m\n",
            "[ 92%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/treelearner/linear_tree_learner.cpp.o\u001b[0m\n",
            "[ 94%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/treelearner/serial_tree_learner.cpp.o\u001b[0m\n",
            "[ 95%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/treelearner/tree_learner.cpp.o\u001b[0m\n",
            "[ 97%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/treelearner/voting_parallel_tree_learner.cpp.o\u001b[0m\n",
            "[ 98%] \u001b[32m\u001b[1mLinking CXX shared library ../lib_lightgbm.so\u001b[0m\n",
            "[ 98%] Built target _lightgbm\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable ../lightgbm\u001b[0m\n",
            "[100%] Built target lightgbm\n",
            "running install\n",
            "running build\n",
            "running build_py\n",
            "INFO:root:Generating grammar tables from /usr/lib/python3.7/lib2to3/Grammar.txt\n",
            "INFO:root:Generating grammar tables from /usr/lib/python3.7/lib2to3/PatternGrammar.txt\n",
            "creating build\n",
            "creating build/lib\n",
            "creating build/lib/lightgbm\n",
            "copying lightgbm/compat.py -> build/lib/lightgbm\n",
            "copying lightgbm/basic.py -> build/lib/lightgbm\n",
            "copying lightgbm/libpath.py -> build/lib/lightgbm\n",
            "copying lightgbm/callback.py -> build/lib/lightgbm\n",
            "copying lightgbm/engine.py -> build/lib/lightgbm\n",
            "copying lightgbm/__init__.py -> build/lib/lightgbm\n",
            "copying lightgbm/dask.py -> build/lib/lightgbm\n",
            "copying lightgbm/plotting.py -> build/lib/lightgbm\n",
            "copying lightgbm/sklearn.py -> build/lib/lightgbm\n",
            "running egg_info\n",
            "creating lightgbm.egg-info\n",
            "writing lightgbm.egg-info/PKG-INFO\n",
            "writing dependency_links to lightgbm.egg-info/dependency_links.txt\n",
            "writing requirements to lightgbm.egg-info/requires.txt\n",
            "writing top-level names to lightgbm.egg-info/top_level.txt\n",
            "writing manifest file 'lightgbm.egg-info/SOURCES.txt'\n",
            "reading manifest template 'MANIFEST.in'\n",
            "no previously-included directories found matching 'build'\n",
            "warning: no files found matching 'LICENSE'\n",
            "warning: no files found matching '*.txt'\n",
            "warning: no files found matching '*.so' under directory 'lightgbm'\n",
            "warning: no files found matching 'compile/CMakeLists.txt'\n",
            "warning: no files found matching 'compile/cmake/IntegratedOpenCL.cmake'\n",
            "warning: no files found matching '*.so' under directory 'compile'\n",
            "warning: no files found matching '*.dll' under directory 'compile/Release'\n",
            "warning: no files found matching 'compile/external_libs/compute/CMakeLists.txt'\n",
            "warning: no files found matching '*' under directory 'compile/external_libs/compute/cmake'\n",
            "warning: no files found matching '*' under directory 'compile/external_libs/compute/include'\n",
            "warning: no files found matching '*' under directory 'compile/external_libs/compute/meta'\n",
            "warning: no files found matching 'compile/external_libs/eigen/CMakeLists.txt'\n",
            "warning: no files found matching 'compile/external_libs/eigen/Eigen/Cholesky'\n",
            "warning: no files found matching 'compile/external_libs/eigen/Eigen/Core'\n",
            "warning: no files found matching 'compile/external_libs/eigen/Eigen/Dense'\n",
            "warning: no files found matching 'compile/external_libs/eigen/Eigen/Eigenvalues'\n",
            "warning: no files found matching 'compile/external_libs/eigen/Eigen/Geometry'\n",
            "warning: no files found matching 'compile/external_libs/eigen/Eigen/Householder'\n",
            "warning: no files found matching 'compile/external_libs/eigen/Eigen/Jacobi'\n",
            "warning: no files found matching 'compile/external_libs/eigen/Eigen/LU'\n",
            "warning: no files found matching 'compile/external_libs/eigen/Eigen/QR'\n",
            "warning: no files found matching 'compile/external_libs/eigen/Eigen/SVD'\n",
            "warning: no files found matching '*' under directory 'compile/external_libs/eigen/Eigen/src/Cholesky'\n",
            "warning: no files found matching '*' under directory 'compile/external_libs/eigen/Eigen/src/Core'\n",
            "warning: no files found matching '*' under directory 'compile/external_libs/eigen/Eigen/src/Eigenvalues'\n",
            "warning: no files found matching '*' under directory 'compile/external_libs/eigen/Eigen/src/Geometry'\n",
            "warning: no files found matching '*' under directory 'compile/external_libs/eigen/Eigen/src/Householder'\n",
            "warning: no files found matching '*' under directory 'compile/external_libs/eigen/Eigen/src/Jacobi'\n",
            "warning: no files found matching '*' under directory 'compile/external_libs/eigen/Eigen/src/LU'\n",
            "warning: no files found matching '*' under directory 'compile/external_libs/eigen/Eigen/src/misc'\n",
            "warning: no files found matching '*' under directory 'compile/external_libs/eigen/Eigen/src/plugins'\n",
            "warning: no files found matching '*' under directory 'compile/external_libs/eigen/Eigen/src/QR'\n",
            "warning: no files found matching '*' under directory 'compile/external_libs/eigen/Eigen/src/SVD'\n",
            "warning: no files found matching 'compile/external_libs/fast_double_parser/CMakeLists.txt'\n",
            "warning: no files found matching 'compile/external_libs/fast_double_parser/LICENSE'\n",
            "warning: no files found matching 'compile/external_libs/fast_double_parser/LICENSE.BSL'\n",
            "warning: no files found matching '*' under directory 'compile/external_libs/fast_double_parser/include'\n",
            "warning: no files found matching 'compile/external_libs/fmt/CMakeLists.txt'\n",
            "warning: no files found matching 'compile/external_libs/fmt/LICENSE.rst'\n",
            "warning: no files found matching '*' under directory 'compile/external_libs/fmt/include'\n",
            "warning: no files found matching '*' under directory 'compile/include'\n",
            "warning: no files found matching '*' under directory 'compile/src'\n",
            "warning: no files found matching 'LightGBM.sln' under directory 'compile/windows'\n",
            "warning: no files found matching 'LightGBM.vcxproj' under directory 'compile/windows'\n",
            "warning: no files found matching '*.dll' under directory 'compile/windows/x64/DLL'\n",
            "warning: no previously-included files matching '*.py[co]' found anywhere in distribution\n",
            "warning: no previously-included files found matching 'compile/external_libs/compute/.git'\n",
            "writing manifest file 'lightgbm.egg-info/SOURCES.txt'\n",
            "copying lightgbm/VERSION.txt -> build/lib/lightgbm\n",
            "running install_lib\n",
            "creating /usr/local/lib/python3.7/dist-packages/lightgbm\n",
            "copying build/lib/lightgbm/VERSION.txt -> /usr/local/lib/python3.7/dist-packages/lightgbm\n",
            "copying build/lib/lightgbm/compat.py -> /usr/local/lib/python3.7/dist-packages/lightgbm\n",
            "copying build/lib/lightgbm/basic.py -> /usr/local/lib/python3.7/dist-packages/lightgbm\n",
            "copying build/lib/lightgbm/libpath.py -> /usr/local/lib/python3.7/dist-packages/lightgbm\n",
            "copying build/lib/lightgbm/callback.py -> /usr/local/lib/python3.7/dist-packages/lightgbm\n",
            "copying build/lib/lightgbm/engine.py -> /usr/local/lib/python3.7/dist-packages/lightgbm\n",
            "copying build/lib/lightgbm/__init__.py -> /usr/local/lib/python3.7/dist-packages/lightgbm\n",
            "copying build/lib/lightgbm/dask.py -> /usr/local/lib/python3.7/dist-packages/lightgbm\n",
            "copying build/lib/lightgbm/plotting.py -> /usr/local/lib/python3.7/dist-packages/lightgbm\n",
            "copying build/lib/lightgbm/sklearn.py -> /usr/local/lib/python3.7/dist-packages/lightgbm\n",
            "INFO:LightGBM:Installing lib_lightgbm from: ['/content/LightGBM/lib_lightgbm.so']\n",
            "copying /content/LightGBM/lib_lightgbm.so -> /usr/local/lib/python3.7/dist-packages/lightgbm\n",
            "byte-compiling /usr/local/lib/python3.7/dist-packages/lightgbm/compat.py to compat.cpython-37.pyc\n",
            "byte-compiling /usr/local/lib/python3.7/dist-packages/lightgbm/basic.py to basic.cpython-37.pyc\n",
            "byte-compiling /usr/local/lib/python3.7/dist-packages/lightgbm/libpath.py to libpath.cpython-37.pyc\n",
            "byte-compiling /usr/local/lib/python3.7/dist-packages/lightgbm/callback.py to callback.cpython-37.pyc\n",
            "byte-compiling /usr/local/lib/python3.7/dist-packages/lightgbm/engine.py to engine.cpython-37.pyc\n",
            "byte-compiling /usr/local/lib/python3.7/dist-packages/lightgbm/__init__.py to __init__.cpython-37.pyc\n",
            "byte-compiling /usr/local/lib/python3.7/dist-packages/lightgbm/dask.py to dask.cpython-37.pyc\n",
            "byte-compiling /usr/local/lib/python3.7/dist-packages/lightgbm/plotting.py to plotting.cpython-37.pyc\n",
            "byte-compiling /usr/local/lib/python3.7/dist-packages/lightgbm/sklearn.py to sklearn.cpython-37.pyc\n",
            "running install_egg_info\n",
            "Copying lightgbm.egg-info to /usr/local/lib/python3.7/dist-packages/lightgbm-3.2.1.99-py3.7.egg-info\n",
            "running install_scripts\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98OZybwarUFw"
      },
      "source": [
        "Other untried option\n",
        "\n",
        "```!pip install lightgbm --install-option=--gpu```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mpnz2sLTzMqA",
        "outputId": "c3b5776a-b4a6-4456-ef5a-681f4a994492"
      },
      "source": [
        "# Next two lines are for Google Colab GPU and working with AutoGluon\n",
        "!pip uninstall -y mkl\n",
        "!pip install --upgrade mxnet-cu100\n",
        "\n",
        "!pip install optuna"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: mkl 2019.0\n",
            "Uninstalling mkl-2019.0:\n",
            "  Successfully uninstalled mkl-2019.0\n",
            "Collecting mxnet-cu100\n",
            "  Downloading mxnet_cu100-1.8.0.post0-py2.py3-none-manylinux2014_x86_64.whl (352.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 352.6 MB 5.6 kB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.7/dist-packages (from mxnet-cu100) (2.23.0)\n",
            "Collecting graphviz<0.9.0,>=0.8.1\n",
            "  Downloading graphviz-0.8.4-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: numpy<2.0.0,>1.16.0 in /usr/local/lib/python3.7/dist-packages (from mxnet-cu100) (1.19.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet-cu100) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet-cu100) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet-cu100) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet-cu100) (2021.5.30)\n",
            "Installing collected packages: graphviz, mxnet-cu100\n",
            "  Attempting uninstall: graphviz\n",
            "    Found existing installation: graphviz 0.10.1\n",
            "    Uninstalling graphviz-0.10.1:\n",
            "      Successfully uninstalled graphviz-0.10.1\n",
            "Successfully installed graphviz-0.8.4 mxnet-cu100-1.8.0.post0\n",
            "Collecting optuna\n",
            "  Downloading optuna-2.9.1-py3-none-any.whl (302 kB)\n",
            "\u001b[K     |████████████████████████████████| 302 kB 12.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy!=1.4.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (1.4.1)\n",
            "Collecting cmaes>=0.8.2\n",
            "  Downloading cmaes-0.8.2-py3-none-any.whl (15 kB)\n",
            "Collecting colorlog\n",
            "  Downloading colorlog-6.4.1-py2.py3-none-any.whl (11 kB)\n",
            "Collecting cliff\n",
            "  Downloading cliff-3.9.0-py3-none-any.whl (80 kB)\n",
            "\u001b[K     |████████████████████████████████| 80 kB 11.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (21.0)\n",
            "Collecting alembic\n",
            "  Downloading alembic-1.7.3-py3-none-any.whl (208 kB)\n",
            "\u001b[K     |████████████████████████████████| 208 kB 79.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from optuna) (4.62.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from optuna) (3.13)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from optuna) (1.19.5)\n",
            "Requirement already satisfied: sqlalchemy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (1.4.23)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->optuna) (2.4.7)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna) (4.8.1)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna) (1.1.1)\n",
            "Collecting Mako\n",
            "  Downloading Mako-1.1.5-py2.py3-none-any.whl (75 kB)\n",
            "\u001b[K     |████████████████████████████████| 75 kB 4.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from alembic->optuna) (5.2.2)\n",
            "Collecting cmd2>=1.0.0\n",
            "  Downloading cmd2-2.2.0-py3-none-any.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 93.9 MB/s \n",
            "\u001b[?25hCollecting pbr!=2.1.0,>=2.0.0\n",
            "  Downloading pbr-5.6.0-py2.py3-none-any.whl (111 kB)\n",
            "\u001b[K     |████████████████████████████████| 111 kB 98.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PrettyTable>=0.7.2 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna) (2.2.0)\n",
            "Collecting autopage>=0.4.0\n",
            "  Downloading autopage-0.4.0-py3-none-any.whl (20 kB)\n",
            "Collecting stevedore>=2.0.1\n",
            "  Downloading stevedore-3.4.0-py3-none-any.whl (49 kB)\n",
            "\u001b[K     |████████████████████████████████| 49 kB 7.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs>=16.3.0 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (21.2.0)\n",
            "Collecting colorama>=0.3.7\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (3.7.4.3)\n",
            "Requirement already satisfied: wcwidth>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (0.2.5)\n",
            "Collecting pyperclip>=1.6\n",
            "  Downloading pyperclip-1.8.2.tar.gz (20 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->sqlalchemy>=1.1.0->optuna) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from Mako->alembic->optuna) (2.0.1)\n",
            "Building wheels for collected packages: pyperclip\n",
            "  Building wheel for pyperclip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyperclip: filename=pyperclip-1.8.2-py3-none-any.whl size=11136 sha256=5776c43d764c1215150f047aec941d6e5de4d67fb32fee42c2c6a82bff328b18\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/18/84/8f69f8b08169c7bae2dde6bd7daf0c19fca8c8e500ee620a28\n",
            "Successfully built pyperclip\n",
            "Installing collected packages: pyperclip, pbr, colorama, stevedore, Mako, cmd2, autopage, colorlog, cmaes, cliff, alembic, optuna\n",
            "Successfully installed Mako-1.1.5 alembic-1.7.3 autopage-0.4.0 cliff-3.9.0 cmaes-0.8.2 cmd2-2.2.0 colorama-0.4.4 colorlog-6.4.1 optuna-2.9.1 pbr-5.6.0 pyperclip-1.8.2 stevedore-3.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xtHrv_bzyxIe"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w42DuU-QzcjC"
      },
      "source": [
        "import random\n",
        "import os\n",
        "from google.colab import output\n",
        "import optuna\n",
        "\n",
        "\n",
        "from lightgbm import LGBMClassifier\n",
        "import lightgbm as lgb\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.ensemble import StackingRegressor\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
        "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
        "import pickle\n",
        "import numpy as np\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "#importing all the plot functions\n",
        "from optuna.visualization import plot_edf\n",
        "from optuna.visualization import plot_optimization_history\n",
        "from optuna.visualization import plot_parallel_coordinate\n",
        "from optuna.visualization import plot_param_importances\n",
        "from optuna.visualization import plot_slice\n",
        "\n",
        "import logging"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rad7vjlwzbhS"
      },
      "source": [
        "## Google Drive Mount\n",
        "\n",
        "* This is used to save models and output\n",
        "* Note that for this Kaggle competition, the train.csv and test.csv are quite large and take a while to upload and store."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4EhprUn0SUP",
        "outputId": "eaf5b381-8fcb-4893-b371-006f3af5d7be"
      },
      "source": [
        "working_dir = 'MyDrive/kaggle/202109'\n",
        "drive_dir = '/content/drive/'\n",
        "model_dir = 'models'\n",
        "output_dir = 'output'\n",
        "from google.colab import drive\n",
        "drive.mount(drive_dir, force_remount=True)  # Do this once only\n",
        "\n",
        "mounted_drive = f'{drive_dir}/{working_dir}' #''\n",
        "os.chdir(mounted_drive)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXDkBnH2IT7f"
      },
      "source": [
        "os.environ['KAGGLE_CONFIG_DIR'] = \"/content/drive/MyDrive/.kaggle\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzVf964IKAex"
      },
      "source": [
        "If you want to directly download the kaggle files to your Google Colab environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aB27i2me0UCm"
      },
      "source": [
        "From https://www.analyticsvidhya.com/blog/2021/05/10-colab-tips-and-hacks-for-efficient-use-of-it/\n",
        "\n",
        "https://github.com/Kaggle/kaggle-api\n",
        "\n",
        "You can pull the datasets directly from Kaggle\n",
        "1.  Go to your account page on kaggle.com and scroll to API to get your token\n",
        "2.  Download the token \n",
        "3.  Copy it to Google Drive - most people put it in the .kaggle directory\n",
        "4.  Set enviornment variable KAGGLE_CONFIG_DIR\n",
        "5.  ```\n",
        "! pip install kaggle\n",
        "kaggle competitions download -c tabular-playground-series-sep-2021```\n",
        "6.  You may need to add logic to move the files where you want them"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tosSRsJ9CaW6"
      },
      "source": [
        "#!pip install kaggle\n",
        "#!kaggle competitions download -c tabular-playground-series-sep-2021"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TgcmQobIGLXM"
      },
      "source": [
        "pd.set_option(\"display.max_rows\", 999)\n",
        "pd.set_option(\"display.max_columns\", 999)\n",
        "\n",
        "# Read csv files\n",
        "df = pd.read_csv('./train.csv')\n",
        "test_df=pd.read_csv('./test.csv')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbRjZYL1G3LS"
      },
      "source": [
        "## Logging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PD8-eSaIG5Io"
      },
      "source": [
        "# Right now this is not working as expected, so this is pending\n",
        "#logging.basicConfig(filename='./artifacts/Kaggle_202109_lgbm.log',level=logging.INFO)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocyMoq1bKTCh"
      },
      "source": [
        "# Massaging Data\n",
        "\n",
        "These are taken from Kaggle notebooks (see commented code)\n",
        "\n",
        "\n",
        "1.   Memory optimization \n",
        "     * scans each column and based on the numeric range, sets the type to the one that requires the least memory\n",
        "     * improves performance because the datasets are very large\n",
        "2.   Imputation\n",
        "     * uses sklearn's iterative imputer which approximates the missing values based on \"nearby\" values.\n",
        "     * improves scores\n",
        "     * note that it takes a while, so for this notebook, the massaged data is being saved locally\n",
        "3.   additional columns are added to indicate the number of rows missing data and the missing feature columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QhXTLAvcKO13"
      },
      "source": [
        "RUN_MASSAGE = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mfAhNkcK6HC"
      },
      "source": [
        "# https://www.kaggle.com/lucamassaron/autogluon-for-tabular-playground-sep-2021\n",
        "# Derived from the original script https://www.kaggle.com/gemartin/load-data-reduce-memory-usage \n",
        "# by Guillaume Martin\n",
        "\n",
        "# This checks the range of values in each column and resets the type \n",
        "\n",
        "def reduce_mem_usage(df, verbose=True):\n",
        "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
        "    start_mem = df.memory_usage().sum() / 1024**2    \n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtypes\n",
        "        if col_type in numerics:\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "            if str(col_type)[:3] == 'int':\n",
        "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                    df[col] = df[col].astype(np.int8)\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                    df[col] = df[col].astype(np.int64)  \n",
        "            else:\n",
        "                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                else:\n",
        "                    df[col] = df[col].astype(np.float64)    \n",
        "    end_mem = df.memory_usage().sum() / 1024**2\n",
        "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d84BDR5WK-hA",
        "outputId": "b622a931-1a32-4b9e-973e-2ca3bdff693c"
      },
      "source": [
        "df = reduce_mem_usage(df)\n",
        "test_df = reduce_mem_usage(test_df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mem. usage decreased to 435.76 Mb (50.3% reduction)\n",
            "Mem. usage decreased to 224.01 Mb (50.0% reduction)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBfy5QSrLCKe"
      },
      "source": [
        "if RUN_MASSAGE:\n",
        "  X = df.drop(columns = ['id', 'claim'])\n",
        "  test_X = test_df.drop(columns = ['id'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92gbTRH8LGBm"
      },
      "source": [
        "#https://www.kaggle.com/lucamassaron/autogluon-for-tabular-playground-sep-2021\n",
        "# Reference: https://www.kaggle.com/hsuchialun/tps-lightgbm-kfold\n",
        "if RUN_MASSAGE:\n",
        "    feats = list(X.columns[:-1])\n",
        "\n",
        "    X['n_row_missing'] = X[feats].isna().sum(axis=1)\n",
        "    test_X['n_row_missing'] = test_X[feats].isna().sum(axis=1)\n",
        "\n",
        "    X['row_std'] = X[feats].std(axis=1)\n",
        "    test_X['row_std'] = test_X[feats].std(axis=1)\n",
        "\n",
        "    feats += ['n_row_missing', 'row_std']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qeH1_GiSLKfe"
      },
      "source": [
        "# MULTIVARIATE ITERATIVE IMPUTATION\n",
        "\n",
        "if RUN_MASSAGE:\n",
        "    \n",
        "    imp = IterativeImputer(max_iter=10, random_state=0)\n",
        "    imp.fit(X[feats].sample(n=10_000))\n",
        "\n",
        "    X[feats] = imp.transform(X[feats])\n",
        "    test_X[feats] = imp.transform(test_X[feats])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efRFARZuLN0Q"
      },
      "source": [
        "if RUN_MASSAGE:\n",
        "    pd.DataFrame(X).to_csv('./reduced_mem_imputed.csv', index=False)\n",
        "    pd.DataFrame(test_X).to_csv('./reduced_mem_imputesd_test.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSLhoaKlLRUH"
      },
      "source": [
        "X = pd.read_csv('./reduced_mem_imputed.csv')\n",
        "test_X = pd.read_csv('./reduced_mem_imputesd_test.csv') # this is the Kaggle test dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMqG8qW0LZ2L"
      },
      "source": [
        "y = df[['claim']]\n",
        "N_SPLITS = 5 # if you're using cross_score_validation\n",
        "EARLY_STOPPING_ROUNDS = 400\n",
        "SEED = 42\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZuM-ELfrLgW-"
      },
      "source": [
        "Optuna \n",
        "* study - this is the tuning done over several iterations of different hyperparameters\n",
        "* trial - this is the individual evaluation/trial of selected hyperparameters\n",
        "* objective function - defines the logic that Optuna will use for a single trial\n",
        "* callback - allows actions after the objective function has returned the ranking metric to Optuna.  \n",
        "\n",
        "At this point Optuna does not save the best model for use after the study.  Instead it will return the best parameters of the study.  However the study can take a long time, so a callback is used to save the best model (as of yet) to a file.\n",
        "\n",
        "**Note that there should be a sample file called best_{model_type} which will be replaced by the best model of the study.  Right now the job will err out, if the model isn't there. ** (And yes, more code can be written to automatically create that file or check it's existence, but it hasn't been done yet)\n",
        "\n",
        "Due to the large training and test datasets, the objective function has been modified to work with train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZvKwVsTYLasw"
      },
      "source": [
        "def lgbm_objective(trial):\n",
        "   \n",
        "    # hyperparameter setting, trial.suggest_uniform will suggest uniform hyperparameter\n",
        "    \n",
        "    \n",
        "    kfolds = KFold(n_splits=N_SPLITS, shuffle=True)\n",
        "    \n",
        "    params = {\n",
        "        \"metric\": \"AUC\",\n",
        "        \"verbosity\": 0,\n",
        "        \"device\": \"gpu\",\n",
        "        \"boosting_type\": trial.suggest_categorical('boosting_type', ['gbdt']), # , 'dart'\n",
        "        \"subsample\": trial.suggest_uniform('subsample', 0.5, 0.6),\n",
        "        \"learning_rate\": trial.suggest_float('eta', 0.005, 0.015),\n",
        "        \"num_iterations\": trial.suggest_int('num_iterations', 1000, 40000),\n",
        "        \"subsample_freq\": 1,\n",
        "        \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 1e-7, 10.0, log=True),\n",
        "        \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1e-7, 10.0, log=True),\n",
        "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 256),\n",
        "        \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.4, 1.0),        \n",
        "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100),\n",
        "    }\n",
        "    \n",
        "    print(f'STARTING Params: {params}')\n",
        "    lgbm_model = LGBMClassifier(**params,random_state=42)\n",
        "    \n",
        "    d_train = lgb.Dataset(X, label=y)\n",
        "\n",
        "    #https://www.kaggle.com/aharless/trying-to-do-k-fold-cv-on-lightgbm\n",
        "    cv_results = lgb.cv(params, d_train, early_stopping_rounds=EARLY_STOPPING_ROUNDS, nfold=N_SPLITS, metrics = 'auc', seed=42)\n",
        "    \n",
        "    #print(cv_results)\n",
        "    #results['auc-mean'][-1]\n",
        "    auc = cv_results['auc-mean']\n",
        "    print('Trial LGBM AUC',np.amax(auc), np.amin(auc), np.sqrt(abs(np.mean(auc))))\n",
        "    filename = f'./models/lgbm_{trial.number}'\n",
        "    \n",
        "    pickle.dump(lgbm_model, open(filename, 'wb'))\n",
        "    \n",
        "    return auc[-1] \n",
        "\n",
        "\n",
        "def lgbm_callback(study, trial):\n",
        "  # from os import remove\n",
        "    filename = f'./models/lgbm_{trial.number}'\n",
        "    \n",
        "    best_model_name = './models/best_lgbm'\n",
        "    if study.best_trial.number == trial.number:\n",
        "        os.remove(best_model_name)\n",
        "        os.rename(filename, best_model_name)\n",
        "    else:\n",
        "        os.remove(filename)\n",
        "    print(f'filename {filename} best_model_name{best_model_name} ; best_trial_number {study.best_trial.number}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "wVr-yAQtZBv-",
        "outputId": "a4a60f1c-102e-4ee9-ed4a-2e5d7730a218"
      },
      "source": [
        "lgbm_study = optuna.create_study(direction='maximize')\n",
        "lgbm_study.optimize(lgbm_objective, n_trials=100, callbacks=[lgbm_callback])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2021-09-17 06:09:29,251]\u001b[0m A new study created in memory with name: no-name-05115bdc-059b-4c5b-b09f-938889ea253a\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:574: UserWarning:\n",
            "\n",
            "Found `num_iterations` in params. Will use it instead of argument\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:578: UserWarning:\n",
            "\n",
            "'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "STARTING Params: {'metric': 'AUC', 'verbosity': 0, 'device': 'gpu', 'boosting_type': 'gbdt', 'subsample': 0.5727063392015879, 'learning_rate': 0.010278674171839874, 'num_iterations': 1569, 'subsample_freq': 1, 'lambda_l1': 0.09211554066717548, 'lambda_l2': 0.12079930458940936, 'num_leaves': 162, 'feature_fraction': 0.7936490021224217, 'min_child_samples': 66}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2021-09-17 06:18:36,096]\u001b[0m Trial 0 finished with value: 0.8146050196152306 and parameters: {'boosting_type': 'gbdt', 'subsample': 0.5727063392015879, 'eta': 0.010278674171839874, 'num_iterations': 1569, 'lambda_l1': 0.09211554066717548, 'lambda_l2': 0.12079930458940936, 'num_leaves': 162, 'feature_fraction': 0.7936490021224217, 'min_child_samples': 66}. Best is trial 0 with value: 0.8146050196152306.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trial LGBM AUC 0.8146050196152306 0.5408594042261294 0.9013952571080929\n",
            "filename ./models/lgbm_0 best_model_name./models/best_lgbm ; best_trial_number 0\n",
            "STARTING Params: {'metric': 'AUC', 'verbosity': 0, 'device': 'gpu', 'boosting_type': 'gbdt', 'subsample': 0.5723089372660386, 'learning_rate': 0.007122586554732636, 'num_iterations': 12974, 'subsample_freq': 1, 'lambda_l1': 0.5901477129019864, 'lambda_l2': 1.8336514748901417e-05, 'num_leaves': 228, 'feature_fraction': 0.5988502385158633, 'min_child_samples': 25}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:574: UserWarning:\n",
            "\n",
            "Found `num_iterations` in params. Will use it instead of argument\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:578: UserWarning:\n",
            "\n",
            "'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "\n",
            "\u001b[32m[I 2021-09-17 06:47:44,328]\u001b[0m Trial 1 finished with value: 0.8149008263401365 and parameters: {'boosting_type': 'gbdt', 'subsample': 0.5723089372660386, 'eta': 0.007122586554732636, 'num_iterations': 12974, 'lambda_l1': 0.5901477129019864, 'lambda_l2': 1.8336514748901417e-05, 'num_leaves': 228, 'feature_fraction': 0.5988502385158633, 'min_child_samples': 25}. Best is trial 1 with value: 0.8149008263401365.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trial LGBM AUC 0.8149008263401365 0.5417103347997344 0.9020221373560633\n",
            "filename ./models/lgbm_1 best_model_name./models/best_lgbm ; best_trial_number 1\n",
            "STARTING Params: {'metric': 'AUC', 'verbosity': 0, 'device': 'gpu', 'boosting_type': 'gbdt', 'subsample': 0.5126628639792093, 'learning_rate': 0.0061624207428288644, 'num_iterations': 22354, 'subsample_freq': 1, 'lambda_l1': 1.671310439721814e-05, 'lambda_l2': 2.901942685738539e-06, 'num_leaves': 48, 'feature_fraction': 0.6174261167967579, 'min_child_samples': 98}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:574: UserWarning:\n",
            "\n",
            "Found `num_iterations` in params. Will use it instead of argument\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:578: UserWarning:\n",
            "\n",
            "'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "\n",
            "\u001b[32m[I 2021-09-17 07:14:50,210]\u001b[0m Trial 2 finished with value: 0.8157054565554273 and parameters: {'boosting_type': 'gbdt', 'subsample': 0.5126628639792093, 'eta': 0.0061624207428288644, 'num_iterations': 22354, 'lambda_l1': 1.671310439721814e-05, 'lambda_l2': 2.901942685738539e-06, 'num_leaves': 48, 'feature_fraction': 0.6174261167967579, 'min_child_samples': 98}. Best is trial 2 with value: 0.8157054565554273.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trial LGBM AUC 0.8157054565554273 0.542873315565455 0.9025394387812226\n",
            "filename ./models/lgbm_2 best_model_name./models/best_lgbm ; best_trial_number 2\n",
            "STARTING Params: {'metric': 'AUC', 'verbosity': 0, 'device': 'gpu', 'boosting_type': 'gbdt', 'subsample': 0.5636025869136195, 'learning_rate': 0.007446201811739935, 'num_iterations': 18029, 'subsample_freq': 1, 'lambda_l1': 0.10208897070895423, 'lambda_l2': 5.0475795956204396e-05, 'num_leaves': 239, 'feature_fraction': 0.4121270422539963, 'min_child_samples': 56}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:574: UserWarning:\n",
            "\n",
            "Found `num_iterations` in params. Will use it instead of argument\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:578: UserWarning:\n",
            "\n",
            "'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "\n",
            "\u001b[32m[I 2021-09-17 07:46:48,838]\u001b[0m Trial 3 finished with value: 0.8152006805647668 and parameters: {'boosting_type': 'gbdt', 'subsample': 0.5636025869136195, 'eta': 0.007446201811739935, 'num_iterations': 18029, 'lambda_l1': 0.10208897070895423, 'lambda_l2': 5.0475795956204396e-05, 'num_leaves': 239, 'feature_fraction': 0.4121270422539963, 'min_child_samples': 56}. Best is trial 2 with value: 0.8157054565554273.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trial LGBM AUC 0.8152006805647668 0.5409810823206036 0.9020990315892111\n",
            "filename ./models/lgbm_3 best_model_name./models/best_lgbm ; best_trial_number 2\n",
            "STARTING Params: {'metric': 'AUC', 'verbosity': 0, 'device': 'gpu', 'boosting_type': 'gbdt', 'subsample': 0.5007860424564303, 'learning_rate': 0.011939819737369326, 'num_iterations': 32718, 'subsample_freq': 1, 'lambda_l1': 0.0017706437978294663, 'lambda_l2': 0.012768654749150034, 'num_leaves': 240, 'feature_fraction': 0.9679895939300337, 'min_child_samples': 12}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:574: UserWarning:\n",
            "\n",
            "Found `num_iterations` in params. Will use it instead of argument\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:578: UserWarning:\n",
            "\n",
            "'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "\n",
            "\u001b[32m[I 2021-09-17 08:03:43,492]\u001b[0m Trial 4 finished with value: 0.8139649390431956 and parameters: {'boosting_type': 'gbdt', 'subsample': 0.5007860424564303, 'eta': 0.011939819737369326, 'num_iterations': 32718, 'lambda_l1': 0.0017706437978294663, 'lambda_l2': 0.012768654749150034, 'num_leaves': 240, 'feature_fraction': 0.9679895939300337, 'min_child_samples': 12}. Best is trial 2 with value: 0.8157054565554273.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trial LGBM AUC 0.8139649390431956 0.7992471449703173 0.9015322132181173\n",
            "filename ./models/lgbm_4 best_model_name./models/best_lgbm ; best_trial_number 2\n",
            "STARTING Params: {'metric': 'AUC', 'verbosity': 0, 'device': 'gpu', 'boosting_type': 'gbdt', 'subsample': 0.596336469617212, 'learning_rate': 0.013885929020462089, 'num_iterations': 6458, 'subsample_freq': 1, 'lambda_l1': 7.752932300497012e-05, 'lambda_l2': 0.04070683497888291, 'num_leaves': 44, 'feature_fraction': 0.8167298690593833, 'min_child_samples': 21}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:574: UserWarning:\n",
            "\n",
            "Found `num_iterations` in params. Will use it instead of argument\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:578: UserWarning:\n",
            "\n",
            "'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "\n",
            "\u001b[32m[I 2021-09-17 08:16:06,461]\u001b[0m Trial 5 finished with value: 0.8150371753786638 and parameters: {'boosting_type': 'gbdt', 'subsample': 0.596336469617212, 'eta': 0.013885929020462089, 'num_iterations': 6458, 'lambda_l1': 7.752932300497012e-05, 'lambda_l2': 0.04070683497888291, 'num_leaves': 44, 'feature_fraction': 0.8167298690593833, 'min_child_samples': 21}. Best is trial 2 with value: 0.8157054565554273.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trial LGBM AUC 0.8150371753786638 0.5444846591223156 0.9022568953903441\n",
            "filename ./models/lgbm_5 best_model_name./models/best_lgbm ; best_trial_number 2\n",
            "STARTING Params: {'metric': 'AUC', 'verbosity': 0, 'device': 'gpu', 'boosting_type': 'gbdt', 'subsample': 0.5574441028378082, 'learning_rate': 0.008646103619500928, 'num_iterations': 21915, 'subsample_freq': 1, 'lambda_l1': 0.08307995752985149, 'lambda_l2': 9.363303949845661e-07, 'num_leaves': 2, 'feature_fraction': 0.9151171172110089, 'min_child_samples': 97}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:574: UserWarning:\n",
            "\n",
            "Found `num_iterations` in params. Will use it instead of argument\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:578: UserWarning:\n",
            "\n",
            "'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "\n",
            "\u001b[32m[I 2021-09-17 08:43:49,043]\u001b[0m Trial 6 finished with value: 0.8136138909271631 and parameters: {'boosting_type': 'gbdt', 'subsample': 0.5574441028378082, 'eta': 0.008646103619500928, 'num_iterations': 21915, 'lambda_l1': 0.08307995752985149, 'lambda_l2': 9.363303949845661e-07, 'num_leaves': 2, 'feature_fraction': 0.9151171172110089, 'min_child_samples': 97}. Best is trial 2 with value: 0.8157054565554273.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trial LGBM AUC 0.8136138909271631 0.507735461595265 0.9006968200604644\n",
            "filename ./models/lgbm_6 best_model_name./models/best_lgbm ; best_trial_number 2\n",
            "STARTING Params: {'metric': 'AUC', 'verbosity': 0, 'device': 'gpu', 'boosting_type': 'gbdt', 'subsample': 0.5679608194386208, 'learning_rate': 0.009257613888755923, 'num_iterations': 15956, 'subsample_freq': 1, 'lambda_l1': 9.402307740854103e-05, 'lambda_l2': 0.5624867081529631, 'num_leaves': 117, 'feature_fraction': 0.46640791076951044, 'min_child_samples': 84}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:574: UserWarning:\n",
            "\n",
            "Found `num_iterations` in params. Will use it instead of argument\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:578: UserWarning:\n",
            "\n",
            "'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "\n",
            "\u001b[32m[I 2021-09-17 09:01:04,823]\u001b[0m Trial 7 finished with value: 0.8153197888872864 and parameters: {'boosting_type': 'gbdt', 'subsample': 0.5679608194386208, 'eta': 0.009257613888755923, 'num_iterations': 15956, 'lambda_l1': 9.402307740854103e-05, 'lambda_l2': 0.5624867081529631, 'num_leaves': 117, 'feature_fraction': 0.46640791076951044, 'min_child_samples': 84}. Best is trial 2 with value: 0.8157054565554273.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trial LGBM AUC 0.8153197888872864 0.5414124940168691 0.9020325513668316\n",
            "filename ./models/lgbm_7 best_model_name./models/best_lgbm ; best_trial_number 2\n",
            "STARTING Params: {'metric': 'AUC', 'verbosity': 0, 'device': 'gpu', 'boosting_type': 'gbdt', 'subsample': 0.5974731049898263, 'learning_rate': 0.014167464784756376, 'num_iterations': 17694, 'subsample_freq': 1, 'lambda_l1': 3.9118381575739135e-07, 'lambda_l2': 0.022750853084495065, 'num_leaves': 24, 'feature_fraction': 0.5938132098123698, 'min_child_samples': 48}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:574: UserWarning:\n",
            "\n",
            "Found `num_iterations` in params. Will use it instead of argument\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:578: UserWarning:\n",
            "\n",
            "'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "\n",
            "\u001b[32m[I 2021-09-17 09:17:00,041]\u001b[0m Trial 8 finished with value: 0.8155502074006629 and parameters: {'boosting_type': 'gbdt', 'subsample': 0.5974731049898263, 'eta': 0.014167464784756376, 'num_iterations': 17694, 'lambda_l1': 3.9118381575739135e-07, 'lambda_l2': 0.022750853084495065, 'num_leaves': 24, 'feature_fraction': 0.5938132098123698, 'min_child_samples': 48}. Best is trial 2 with value: 0.8157054565554273.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trial LGBM AUC 0.8155502074006629 0.5406170344126303 0.9025723561572209\n",
            "filename ./models/lgbm_8 best_model_name./models/best_lgbm ; best_trial_number 2\n",
            "STARTING Params: {'metric': 'AUC', 'verbosity': 0, 'device': 'gpu', 'boosting_type': 'gbdt', 'subsample': 0.53662188838731, 'learning_rate': 0.009060770761417349, 'num_iterations': 13158, 'subsample_freq': 1, 'lambda_l1': 0.00094468386535772, 'lambda_l2': 0.00043148373240762937, 'num_leaves': 99, 'feature_fraction': 0.4097233939419307, 'min_child_samples': 13}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:574: UserWarning:\n",
            "\n",
            "Found `num_iterations` in params. Will use it instead of argument\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:578: UserWarning:\n",
            "\n",
            "'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "\n",
            "\u001b[32m[I 2021-09-17 09:34:00,059]\u001b[0m Trial 9 finished with value: 0.8151673774578466 and parameters: {'boosting_type': 'gbdt', 'subsample': 0.53662188838731, 'eta': 0.009060770761417349, 'num_iterations': 13158, 'lambda_l1': 0.00094468386535772, 'lambda_l2': 0.00043148373240762937, 'num_leaves': 99, 'feature_fraction': 0.4097233939419307, 'min_child_samples': 13}. Best is trial 2 with value: 0.8157054565554273.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trial LGBM AUC 0.8151673774578466 0.5426591887221252 0.9019971822694327\n",
            "filename ./models/lgbm_9 best_model_name./models/best_lgbm ; best_trial_number 2\n",
            "STARTING Params: {'metric': 'AUC', 'verbosity': 0, 'device': 'gpu', 'boosting_type': 'gbdt', 'subsample': 0.5017593506088529, 'learning_rate': 0.005403931043209345, 'num_iterations': 29381, 'subsample_freq': 1, 'lambda_l1': 1.7336997491412977e-07, 'lambda_l2': 1.1829633575532706e-07, 'num_leaves': 69, 'feature_fraction': 0.6573049536503066, 'min_child_samples': 99}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:574: UserWarning:\n",
            "\n",
            "Found `num_iterations` in params. Will use it instead of argument\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:578: UserWarning:\n",
            "\n",
            "'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "\n",
            "\u001b[32m[I 2021-09-17 10:15:07,169]\u001b[0m Trial 10 finished with value: 0.8157128828715831 and parameters: {'boosting_type': 'gbdt', 'subsample': 0.5017593506088529, 'eta': 0.005403931043209345, 'num_iterations': 29381, 'lambda_l1': 1.7336997491412977e-07, 'lambda_l2': 1.1829633575532706e-07, 'num_leaves': 69, 'feature_fraction': 0.6573049536503066, 'min_child_samples': 99}. Best is trial 10 with value: 0.8157128828715831.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trial LGBM AUC 0.8157128828715831 0.5426471813237931 0.9026632501551617\n",
            "filename ./models/lgbm_10 best_model_name./models/best_lgbm ; best_trial_number 10\n",
            "STARTING Params: {'metric': 'AUC', 'verbosity': 0, 'device': 'gpu', 'boosting_type': 'gbdt', 'subsample': 0.5019406654813531, 'learning_rate': 0.005172895979064616, 'num_iterations': 29362, 'subsample_freq': 1, 'lambda_l1': 1.5040990005655627e-07, 'lambda_l2': 1.2110130371977854e-07, 'num_leaves': 69, 'feature_fraction': 0.6160469875710929, 'min_child_samples': 96}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:574: UserWarning:\n",
            "\n",
            "Found `num_iterations` in params. Will use it instead of argument\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:578: UserWarning:\n",
            "\n",
            "'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "\n",
            "\u001b[32m[I 2021-09-17 10:50:43,540]\u001b[0m Trial 11 finished with value: 0.8157812451608006 and parameters: {'boosting_type': 'gbdt', 'subsample': 0.5019406654813531, 'eta': 0.005172895979064616, 'num_iterations': 29362, 'lambda_l1': 1.5040990005655627e-07, 'lambda_l2': 1.2110130371977854e-07, 'num_leaves': 69, 'feature_fraction': 0.6160469875710929, 'min_child_samples': 96}. Best is trial 11 with value: 0.8157812451608006.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trial LGBM AUC 0.8157812451608006 0.5422841630188685 0.9025939998662025\n",
            "filename ./models/lgbm_11 best_model_name./models/best_lgbm ; best_trial_number 11\n",
            "STARTING Params: {'metric': 'AUC', 'verbosity': 0, 'device': 'gpu', 'boosting_type': 'gbdt', 'subsample': 0.5259194267722721, 'learning_rate': 0.00527276508862559, 'num_iterations': 33609, 'subsample_freq': 1, 'lambda_l1': 1.1435222044985643e-07, 'lambda_l2': 1.0122585280364721e-07, 'num_leaves': 79, 'feature_fraction': 0.6987533311023189, 'min_child_samples': 78}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:574: UserWarning:\n",
            "\n",
            "Found `num_iterations` in params. Will use it instead of argument\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:578: UserWarning:\n",
            "\n",
            "'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "\n",
            "\u001b[32m[I 2021-09-17 11:21:17,207]\u001b[0m Trial 12 finished with value: 0.8155551295048993 and parameters: {'boosting_type': 'gbdt', 'subsample': 0.5259194267722721, 'eta': 0.00527276508862559, 'num_iterations': 33609, 'lambda_l1': 1.1435222044985643e-07, 'lambda_l2': 1.0122585280364721e-07, 'num_leaves': 79, 'feature_fraction': 0.6987533311023189, 'min_child_samples': 78}. Best is trial 11 with value: 0.8157812451608006.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trial LGBM AUC 0.8155551295048993 0.542531854372488 0.9024435064906074\n",
            "filename ./models/lgbm_12 best_model_name./models/best_lgbm ; best_trial_number 11\n",
            "STARTING Params: {'metric': 'AUC', 'verbosity': 0, 'device': 'gpu', 'boosting_type': 'gbdt', 'subsample': 0.5024037022003133, 'learning_rate': 0.005133960323636035, 'num_iterations': 27946, 'subsample_freq': 1, 'lambda_l1': 2.2942740302876687e-06, 'lambda_l2': 1.0789339643868345e-07, 'num_leaves': 160, 'feature_fraction': 0.7118508416702233, 'min_child_samples': 80}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:574: UserWarning:\n",
            "\n",
            "Found `num_iterations` in params. Will use it instead of argument\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:578: UserWarning:\n",
            "\n",
            "'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "\n",
            "\u001b[32m[I 2021-09-17 12:03:46,516]\u001b[0m Trial 13 finished with value: 0.8154185353062167 and parameters: {'boosting_type': 'gbdt', 'subsample': 0.5024037022003133, 'eta': 0.005133960323636035, 'num_iterations': 27946, 'lambda_l1': 2.2942740302876687e-06, 'lambda_l2': 1.0789339643868345e-07, 'num_leaves': 160, 'feature_fraction': 0.7118508416702233, 'min_child_samples': 80}. Best is trial 11 with value: 0.8157812451608006.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trial LGBM AUC 0.8154185353062167 0.540987963515118 0.9024338558988458\n",
            "filename ./models/lgbm_13 best_model_name./models/best_lgbm ; best_trial_number 11\n",
            "STARTING Params: {'metric': 'AUC', 'verbosity': 0, 'device': 'gpu', 'boosting_type': 'gbdt', 'subsample': 0.524210217320848, 'learning_rate': 0.006851098465148982, 'num_iterations': 39554, 'subsample_freq': 1, 'lambda_l1': 1.3200559470570801e-06, 'lambda_l2': 1.4405933920487736e-06, 'num_leaves': 78, 'feature_fraction': 0.5405579875345325, 'min_child_samples': 98}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:574: UserWarning:\n",
            "\n",
            "Found `num_iterations` in params. Will use it instead of argument\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:578: UserWarning:\n",
            "\n",
            "'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "\n",
            "\u001b[32m[I 2021-09-17 12:31:05,308]\u001b[0m Trial 14 finished with value: 0.8156249758574171 and parameters: {'boosting_type': 'gbdt', 'subsample': 0.524210217320848, 'eta': 0.006851098465148982, 'num_iterations': 39554, 'lambda_l1': 1.3200559470570801e-06, 'lambda_l2': 1.4405933920487736e-06, 'num_leaves': 78, 'feature_fraction': 0.5405579875345325, 'min_child_samples': 98}. Best is trial 11 with value: 0.8157812451608006.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trial LGBM AUC 0.8156249758574171 0.5430916867443718 0.902463268509632\n",
            "filename ./models/lgbm_14 best_model_name./models/best_lgbm ; best_trial_number 11\n",
            "STARTING Params: {'metric': 'AUC', 'verbosity': 0, 'device': 'gpu', 'boosting_type': 'gbdt', 'subsample': 0.5402444390845318, 'learning_rate': 0.011092160577207397, 'num_iterations': 26730, 'subsample_freq': 1, 'lambda_l1': 7.735996059669064e-06, 'lambda_l2': 0.0006878621166496951, 'num_leaves': 134, 'feature_fraction': 0.7001145249591282, 'min_child_samples': 46}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:574: UserWarning:\n",
            "\n",
            "Found `num_iterations` in params. Will use it instead of argument\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:578: UserWarning:\n",
            "\n",
            "'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "\n",
            "\u001b[32m[I 2021-09-17 12:47:51,281]\u001b[0m Trial 15 finished with value: 0.8147710856980008 and parameters: {'boosting_type': 'gbdt', 'subsample': 0.5402444390845318, 'eta': 0.011092160577207397, 'num_iterations': 26730, 'lambda_l1': 7.735996059669064e-06, 'lambda_l2': 0.0006878621166496951, 'num_leaves': 134, 'feature_fraction': 0.7001145249591282, 'min_child_samples': 46}. Best is trial 11 with value: 0.8157812451608006.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trial LGBM AUC 0.8147710856980008 0.5417071441995484 0.9020591666253103\n",
            "filename ./models/lgbm_15 best_model_name./models/best_lgbm ; best_trial_number 11\n",
            "STARTING Params: {'metric': 'AUC', 'verbosity': 0, 'device': 'gpu', 'boosting_type': 'gbdt', 'subsample': 0.5171274348797861, 'learning_rate': 0.008004950128349248, 'num_iterations': 35800, 'subsample_freq': 1, 'lambda_l1': 1.0533987569539099e-07, 'lambda_l2': 2.3466610467039646e-05, 'num_leaves': 67, 'feature_fraction': 0.5332882298873162, 'min_child_samples': 67}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:574: UserWarning:\n",
            "\n",
            "Found `num_iterations` in params. Will use it instead of argument\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:578: UserWarning:\n",
            "\n",
            "'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "\n",
            "\u001b[32m[I 2021-09-17 13:14:33,184]\u001b[0m Trial 16 finished with value: 0.8154891262308869 and parameters: {'boosting_type': 'gbdt', 'subsample': 0.5171274348797861, 'eta': 0.008004950128349248, 'num_iterations': 35800, 'lambda_l1': 1.0533987569539099e-07, 'lambda_l2': 2.3466610467039646e-05, 'num_leaves': 67, 'feature_fraction': 0.5332882298873162, 'min_child_samples': 67}. Best is trial 11 with value: 0.8157812451608006.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trial LGBM AUC 0.8154891262308869 0.5420146534003578 0.9024785429380229\n",
            "filename ./models/lgbm_16 best_model_name./models/best_lgbm ; best_trial_number 11\n",
            "STARTING Params: {'metric': 'AUC', 'verbosity': 0, 'device': 'gpu', 'boosting_type': 'gbdt', 'subsample': 0.5115508515462496, 'learning_rate': 0.005985986840097134, 'num_iterations': 27518, 'subsample_freq': 1, 'lambda_l1': 0.0022511125488097485, 'lambda_l2': 6.948364002321384e-07, 'num_leaves': 144, 'feature_fraction': 0.7917098220797911, 'min_child_samples': 89}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:574: UserWarning:\n",
            "\n",
            "Found `num_iterations` in params. Will use it instead of argument\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:578: UserWarning:\n",
            "\n",
            "'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "\n",
            "\u001b[32m[I 2021-09-17 13:49:03,545]\u001b[0m Trial 17 finished with value: 0.8154202854446935 and parameters: {'boosting_type': 'gbdt', 'subsample': 0.5115508515462496, 'eta': 0.005985986840097134, 'num_iterations': 27518, 'lambda_l1': 0.0022511125488097485, 'lambda_l2': 6.948364002321384e-07, 'num_leaves': 144, 'feature_fraction': 0.7917098220797911, 'min_child_samples': 89}. Best is trial 11 with value: 0.8157812451608006.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trial LGBM AUC 0.8154202854446935 0.5425331586381169 0.9024076580440012\n",
            "filename ./models/lgbm_17 best_model_name./models/best_lgbm ; best_trial_number 11\n",
            "STARTING Params: {'metric': 'AUC', 'verbosity': 0, 'device': 'gpu', 'boosting_type': 'gbdt', 'subsample': 0.53833874888673, 'learning_rate': 0.012604279892076194, 'num_iterations': 30267, 'subsample_freq': 1, 'lambda_l1': 1.3463543290939029e-06, 'lambda_l2': 3.612743718988148e-06, 'num_leaves': 195, 'feature_fraction': 0.6539172744436095, 'min_child_samples': 71}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:574: UserWarning:\n",
            "\n",
            "Found `num_iterations` in params. Will use it instead of argument\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:578: UserWarning:\n",
            "\n",
            "'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "\n",
            "\u001b[32m[I 2021-09-17 14:00:54,279]\u001b[0m Trial 18 finished with value: 0.8145462725082278 and parameters: {'boosting_type': 'gbdt', 'subsample': 0.53833874888673, 'eta': 0.012604279892076194, 'num_iterations': 30267, 'lambda_l1': 1.3463543290939029e-06, 'lambda_l2': 3.612743718988148e-06, 'num_leaves': 195, 'feature_fraction': 0.6539172744436095, 'min_child_samples': 71}. Best is trial 11 with value: 0.8157812451608006.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trial LGBM AUC 0.8145462725082278 0.542139713427313 0.9015690757029317\n",
            "filename ./models/lgbm_18 best_model_name./models/best_lgbm ; best_trial_number 11\n",
            "STARTING Params: {'metric': 'AUC', 'verbosity': 0, 'device': 'gpu', 'boosting_type': 'gbdt', 'subsample': 0.526377224809406, 'learning_rate': 0.0062010318996095016, 'num_iterations': 39614, 'subsample_freq': 1, 'lambda_l1': 9.79895795913152, 'lambda_l2': 2.6293226443181807, 'num_leaves': 100, 'feature_fraction': 0.5228755040643674, 'min_child_samples': 40}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:574: UserWarning:\n",
            "\n",
            "Found `num_iterations` in params. Will use it instead of argument\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:578: UserWarning:\n",
            "\n",
            "'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "\n",
            "\u001b[32m[I 2021-09-17 14:32:52,914]\u001b[0m Trial 19 finished with value: 0.8158655110229824 and parameters: {'boosting_type': 'gbdt', 'subsample': 0.526377224809406, 'eta': 0.0062010318996095016, 'num_iterations': 39614, 'lambda_l1': 9.79895795913152, 'lambda_l2': 2.6293226443181807, 'num_leaves': 100, 'feature_fraction': 0.5228755040643674, 'min_child_samples': 40}. Best is trial 19 with value: 0.8158655110229824.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trial LGBM AUC 0.8158655110229824 0.5422660941957943 0.902297571654655\n",
            "filename ./models/lgbm_19 best_model_name./models/best_lgbm ; best_trial_number 19\n",
            "STARTING Params: {'metric': 'AUC', 'verbosity': 0, 'device': 'gpu', 'boosting_type': 'gbdt', 'subsample': 0.5286474663251459, 'learning_rate': 0.0063520792774278585, 'num_iterations': 39718, 'subsample_freq': 1, 'lambda_l1': 4.024804310483001, 'lambda_l2': 1.0822805234205795, 'num_leaves': 108, 'feature_fraction': 0.49815973796353064, 'min_child_samples': 36}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:574: UserWarning:\n",
            "\n",
            "Found `num_iterations` in params. Will use it instead of argument\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:578: UserWarning:\n",
            "\n",
            "'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "\n",
            "\u001b[32m[I 2021-09-17 15:07:12,974]\u001b[0m Trial 20 finished with value: 0.8159092406767193 and parameters: {'boosting_type': 'gbdt', 'subsample': 0.5286474663251459, 'eta': 0.0063520792774278585, 'num_iterations': 39718, 'lambda_l1': 4.024804310483001, 'lambda_l2': 1.0822805234205795, 'num_leaves': 108, 'feature_fraction': 0.49815973796353064, 'min_child_samples': 36}. Best is trial 20 with value: 0.8159092406767193.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trial LGBM AUC 0.8159092406767193 0.541571955242931 0.9024998119014747\n",
            "filename ./models/lgbm_20 best_model_name./models/best_lgbm ; best_trial_number 20\n",
            "STARTING Params: {'metric': 'AUC', 'verbosity': 0, 'device': 'gpu', 'boosting_type': 'gbdt', 'subsample': 0.5278299681437211, 'learning_rate': 0.006487677079659437, 'num_iterations': 39520, 'subsample_freq': 1, 'lambda_l1': 9.014865740694933, 'lambda_l2': 4.245358888340319, 'num_leaves': 99, 'feature_fraction': 0.5047457136539589, 'min_child_samples': 39}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:574: UserWarning:\n",
            "\n",
            "Found `num_iterations` in params. Will use it instead of argument\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:578: UserWarning:\n",
            "\n",
            "'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "\n",
            "\u001b[32m[I 2021-09-17 15:41:55,248]\u001b[0m Trial 21 finished with value: 0.8159660099828953 and parameters: {'boosting_type': 'gbdt', 'subsample': 0.5278299681437211, 'eta': 0.006487677079659437, 'num_iterations': 39520, 'lambda_l1': 9.014865740694933, 'lambda_l2': 4.245358888340319, 'num_leaves': 99, 'feature_fraction': 0.5047457136539589, 'min_child_samples': 39}. Best is trial 21 with value: 0.8159660099828953.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trial LGBM AUC 0.8159660099828953 0.5418308007020542 0.9024664692417133\n",
            "filename ./models/lgbm_21 best_model_name./models/best_lgbm ; best_trial_number 21\n",
            "STARTING Params: {'metric': 'AUC', 'verbosity': 0, 'device': 'gpu', 'boosting_type': 'gbdt', 'subsample': 0.5476151240163777, 'learning_rate': 0.006737401156176602, 'num_iterations': 39304, 'subsample_freq': 1, 'lambda_l1': 9.085590519618057, 'lambda_l2': 3.7827007575533216, 'num_leaves': 103, 'feature_fraction': 0.5002770482278937, 'min_child_samples': 34}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:574: UserWarning:\n",
            "\n",
            "Found `num_iterations` in params. Will use it instead of argument\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:578: UserWarning:\n",
            "\n",
            "'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "\n",
            "\u001b[32m[I 2021-09-17 16:14:23,204]\u001b[0m Trial 22 finished with value: 0.8158916849005287 and parameters: {'boosting_type': 'gbdt', 'subsample': 0.5476151240163777, 'eta': 0.006737401156176602, 'num_iterations': 39304, 'lambda_l1': 9.085590519618057, 'lambda_l2': 3.7827007575533216, 'num_leaves': 103, 'feature_fraction': 0.5002770482278937, 'min_child_samples': 34}. Best is trial 21 with value: 0.8159660099828953.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trial LGBM AUC 0.8158916849005287 0.5413524224084356 0.9024078459035967\n",
            "filename ./models/lgbm_22 best_model_name./models/best_lgbm ; best_trial_number 21\n",
            "STARTING Params: {'metric': 'AUC', 'verbosity': 0, 'device': 'gpu', 'boosting_type': 'gbdt', 'subsample': 0.5489213569373916, 'learning_rate': 0.008008751277349029, 'num_iterations': 36280, 'subsample_freq': 1, 'lambda_l1': 9.033502306120315, 'lambda_l2': 8.834261990773554, 'num_leaves': 108, 'feature_fraction': 0.48467504526475336, 'min_child_samples': 36}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:574: UserWarning:\n",
            "\n",
            "Found `num_iterations` in params. Will use it instead of argument\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:578: UserWarning:\n",
            "\n",
            "'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "\n",
            "\u001b[32m[I 2021-09-17 16:44:00,396]\u001b[0m Trial 23 finished with value: 0.815836401832903 and parameters: {'boosting_type': 'gbdt', 'subsample': 0.5489213569373916, 'eta': 0.008008751277349029, 'num_iterations': 36280, 'lambda_l1': 9.033502306120315, 'lambda_l2': 8.834261990773554, 'num_leaves': 108, 'feature_fraction': 0.48467504526475336, 'min_child_samples': 36}. Best is trial 21 with value: 0.8159660099828953.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trial LGBM AUC 0.815836401832903 0.5418476631258802 0.9024070581934397\n",
            "filename ./models/lgbm_23 best_model_name./models/best_lgbm ; best_trial_number 21\n",
            "STARTING Params: {'metric': 'AUC', 'verbosity': 0, 'device': 'gpu', 'boosting_type': 'gbdt', 'subsample': 0.5458952203761629, 'learning_rate': 0.006682554523136759, 'num_iterations': 36637, 'subsample_freq': 1, 'lambda_l1': 1.5666594925545594, 'lambda_l2': 0.6403096406624899, 'num_leaves': 172, 'feature_fraction': 0.4850439676652866, 'min_child_samples': 32}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:574: UserWarning:\n",
            "\n",
            "Found `num_iterations` in params. Will use it instead of argument\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:578: UserWarning:\n",
            "\n",
            "'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "\n",
            "\u001b[32m[I 2021-09-17 17:20:50,927]\u001b[0m Trial 24 finished with value: 0.8155692177632708 and parameters: {'boosting_type': 'gbdt', 'subsample': 0.5458952203761629, 'eta': 0.006682554523136759, 'num_iterations': 36637, 'lambda_l1': 1.5666594925545594, 'lambda_l2': 0.6403096406624899, 'num_leaves': 172, 'feature_fraction': 0.4850439676652866, 'min_child_samples': 32}. Best is trial 21 with value: 0.8159660099828953.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trial LGBM AUC 0.8155692177632708 0.5416924113517304 0.9023790854520122\n",
            "filename ./models/lgbm_24 best_model_name./models/best_lgbm ; best_trial_number 21\n",
            "STARTING Params: {'metric': 'AUC', 'verbosity': 0, 'device': 'gpu', 'boosting_type': 'gbdt', 'subsample': 0.5303674526915412, 'learning_rate': 0.00780154540223571, 'num_iterations': 38376, 'subsample_freq': 1, 'lambda_l1': 1.1082998631419592, 'lambda_l2': 9.85949255394303, 'num_leaves': 128, 'feature_fraction': 0.4587223596168046, 'min_child_samples': 57}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:574: UserWarning:\n",
            "\n",
            "Found `num_iterations` in params. Will use it instead of argument\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:578: UserWarning:\n",
            "\n",
            "'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "\n",
            "\u001b[32m[I 2021-09-17 17:43:10,960]\u001b[0m Trial 25 finished with value: 0.8156485926046887 and parameters: {'boosting_type': 'gbdt', 'subsample': 0.5303674526915412, 'eta': 0.00780154540223571, 'num_iterations': 38376, 'lambda_l1': 1.1082998631419592, 'lambda_l2': 9.85949255394303, 'num_leaves': 128, 'feature_fraction': 0.4587223596168046, 'min_child_samples': 57}. Best is trial 21 with value: 0.8159660099828953.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trial LGBM AUC 0.8156485926046887 0.5412928315843264 0.9021179005410486\n",
            "filename ./models/lgbm_25 best_model_name./models/best_lgbm ; best_trial_number 21\n",
            "STARTING Params: {'metric': 'AUC', 'verbosity': 0, 'device': 'gpu', 'boosting_type': 'gbdt', 'subsample': 0.5545466973928422, 'learning_rate': 0.00982204963314236, 'num_iterations': 33736, 'subsample_freq': 1, 'lambda_l1': 2.992990005859538, 'lambda_l2': 0.6459315356526913, 'num_leaves': 93, 'feature_fraction': 0.5619844890273882, 'min_child_samples': 26}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:574: UserWarning:\n",
            "\n",
            "Found `num_iterations` in params. Will use it instead of argument\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:578: UserWarning:\n",
            "\n",
            "'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "\n",
            "\u001b[32m[I 2021-09-17 17:59:17,937]\u001b[0m Trial 26 finished with value: 0.8155040270732249 and parameters: {'boosting_type': 'gbdt', 'subsample': 0.5545466973928422, 'eta': 0.00982204963314236, 'num_iterations': 33736, 'lambda_l1': 2.992990005859538, 'lambda_l2': 0.6459315356526913, 'num_leaves': 93, 'feature_fraction': 0.5619844890273882, 'min_child_samples': 26}. Best is trial 21 with value: 0.8159660099828953.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trial LGBM AUC 0.8155040270732249 0.5428896483306029 0.902032045382859\n",
            "filename ./models/lgbm_26 best_model_name./models/best_lgbm ; best_trial_number 21\n",
            "STARTING Params: {'metric': 'AUC', 'verbosity': 0, 'device': 'gpu', 'boosting_type': 'gbdt', 'subsample': 0.5345397986250907, 'learning_rate': 0.006261299680087225, 'num_iterations': 25145, 'subsample_freq': 1, 'lambda_l1': 0.37980979670350395, 'lambda_l2': 0.0033863472492283125, 'num_leaves': 191, 'feature_fraction': 0.4329496020649507, 'min_child_samples': 41}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:574: UserWarning:\n",
            "\n",
            "Found `num_iterations` in params. Will use it instead of argument\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:578: UserWarning:\n",
            "\n",
            "'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "\n",
            "\u001b[32m[I 2021-09-17 18:30:04,062]\u001b[0m Trial 27 finished with value: 0.8153620432865765 and parameters: {'boosting_type': 'gbdt', 'subsample': 0.5345397986250907, 'eta': 0.006261299680087225, 'num_iterations': 25145, 'lambda_l1': 0.37980979670350395, 'lambda_l2': 0.0033863472492283125, 'num_leaves': 191, 'feature_fraction': 0.4329496020649507, 'min_child_samples': 41}. Best is trial 21 with value: 0.8159660099828953.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trial LGBM AUC 0.8153620432865765 0.5419741912239635 0.9020925306314148\n",
            "filename ./models/lgbm_27 best_model_name./models/best_lgbm ; best_trial_number 21\n",
            "STARTING Params: {'metric': 'AUC', 'verbosity': 0, 'device': 'gpu', 'boosting_type': 'gbdt', 'subsample': 0.5438042512574957, 'learning_rate': 0.008528908753138347, 'num_iterations': 31570, 'subsample_freq': 1, 'lambda_l1': 0.015315221103088441, 'lambda_l2': 0.14928919400138999, 'num_leaves': 123, 'feature_fraction': 0.5079540484159953, 'min_child_samples': 32}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:574: UserWarning:\n",
            "\n",
            "Found `num_iterations` in params. Will use it instead of argument\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:578: UserWarning:\n",
            "\n",
            "'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "\n",
            "\u001b[32m[I 2021-09-17 18:51:23,142]\u001b[0m Trial 28 finished with value: 0.8152354194436203 and parameters: {'boosting_type': 'gbdt', 'subsample': 0.5438042512574957, 'eta': 0.008528908753138347, 'num_iterations': 31570, 'lambda_l1': 0.015315221103088441, 'lambda_l2': 0.14928919400138999, 'num_leaves': 123, 'feature_fraction': 0.5079540484159953, 'min_child_samples': 32}. Best is trial 21 with value: 0.8159660099828953.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trial LGBM AUC 0.8152354194436203 0.5422689528359176 0.9020982564339145\n",
            "filename ./models/lgbm_28 best_model_name./models/best_lgbm ; best_trial_number 21\n",
            "STARTING Params: {'metric': 'AUC', 'verbosity': 0, 'device': 'gpu', 'boosting_type': 'gbdt', 'subsample': 0.5199615493327984, 'learning_rate': 0.007230557119618307, 'num_iterations': 1948, 'subsample_freq': 1, 'lambda_l1': 0.1685291461724972, 'lambda_l2': 1.6960529374481275, 'num_leaves': 150, 'feature_fraction': 0.5654912102280674, 'min_child_samples': 6}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:574: UserWarning:\n",
            "\n",
            "Found `num_iterations` in params. Will use it instead of argument\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:578: UserWarning:\n",
            "\n",
            "'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "\n",
            "\u001b[32m[I 2021-09-17 19:01:53,565]\u001b[0m Trial 29 finished with value: 0.8146042139555598 and parameters: {'boosting_type': 'gbdt', 'subsample': 0.5199615493327984, 'eta': 0.007230557119618307, 'num_iterations': 1948, 'lambda_l1': 0.1685291461724972, 'lambda_l2': 1.6960529374481275, 'num_leaves': 150, 'feature_fraction': 0.5654912102280674, 'min_child_samples': 6}. Best is trial 21 with value: 0.8159660099828953.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trial LGBM AUC 0.8146042139555598 0.54219086785774 0.9011014910957529\n",
            "filename ./models/lgbm_29 best_model_name./models/best_lgbm ; best_trial_number 21\n",
            "STARTING Params: {'metric': 'AUC', 'verbosity': 0, 'device': 'gpu', 'boosting_type': 'gbdt', 'subsample': 0.5561763197913913, 'learning_rate': 0.005938065856971338, 'num_iterations': 35649, 'subsample_freq': 1, 'lambda_l1': 0.009458669230133518, 'lambda_l2': 0.15401873637098, 'num_leaves': 53, 'feature_fraction': 0.7544203019616958, 'min_child_samples': 53}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:574: UserWarning:\n",
            "\n",
            "Found `num_iterations` in params. Will use it instead of argument\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:578: UserWarning:\n",
            "\n",
            "'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "\n",
            "\u001b[32m[I 2021-09-17 19:33:09,453]\u001b[0m Trial 30 finished with value: 0.815625359448352 and parameters: {'boosting_type': 'gbdt', 'subsample': 0.5561763197913913, 'eta': 0.005938065856971338, 'num_iterations': 35649, 'lambda_l1': 0.009458669230133518, 'lambda_l2': 0.15401873637098, 'num_leaves': 53, 'feature_fraction': 0.7544203019616958, 'min_child_samples': 53}. Best is trial 21 with value: 0.8159660099828953.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trial LGBM AUC 0.815625359448352 0.5417016960761527 0.9025494297861603\n",
            "filename ./models/lgbm_30 best_model_name./models/best_lgbm ; best_trial_number 21\n",
            "STARTING Params: {'metric': 'AUC', 'verbosity': 0, 'device': 'gpu', 'boosting_type': 'gbdt', 'subsample': 0.5831588943568513, 'learning_rate': 0.00663997823891355, 'num_iterations': 39786, 'subsample_freq': 1, 'lambda_l1': 9.17935937317507, 'lambda_l2': 2.651315984269685, 'num_leaves': 91, 'feature_fraction': 0.5173521241641071, 'min_child_samples': 41}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:574: UserWarning:\n",
            "\n",
            "Found `num_iterations` in params. Will use it instead of argument\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:578: UserWarning:\n",
            "\n",
            "'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "\n",
            "\u001b[32m[I 2021-09-17 20:05:21,535]\u001b[0m Trial 31 finished with value: 0.8159224372416475 and parameters: {'boosting_type': 'gbdt', 'subsample': 0.5831588943568513, 'eta': 0.00663997823891355, 'num_iterations': 39786, 'lambda_l1': 9.17935937317507, 'lambda_l2': 2.651315984269685, 'num_leaves': 91, 'feature_fraction': 0.5173521241641071, 'min_child_samples': 41}. Best is trial 21 with value: 0.8159660099828953.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trial LGBM AUC 0.8159224372416475 0.5425131193995053 0.9024250385627037\n",
            "filename ./models/lgbm_31 best_model_name./models/best_lgbm ; best_trial_number 21\n",
            "STARTING Params: {'metric': 'AUC', 'verbosity': 0, 'device': 'gpu', 'boosting_type': 'gbdt', 'subsample': 0.5890952322822424, 'learning_rate': 0.006696600376128748, 'num_iterations': 39692, 'subsample_freq': 1, 'lambda_l1': 3.56953478069416, 'lambda_l2': 2.263719207938312, 'num_leaves': 93, 'feature_fraction': 0.501014307073317, 'min_child_samples': 25}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:574: UserWarning:\n",
            "\n",
            "Found `num_iterations` in params. Will use it instead of argument\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:578: UserWarning:\n",
            "\n",
            "'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "\n",
            "\u001b[32m[I 2021-09-17 20:42:07,155]\u001b[0m Trial 32 finished with value: 0.8158985730677593 and parameters: {'boosting_type': 'gbdt', 'subsample': 0.5890952322822424, 'eta': 0.006696600376128748, 'num_iterations': 39692, 'lambda_l1': 3.56953478069416, 'lambda_l2': 2.263719207938312, 'num_leaves': 93, 'feature_fraction': 0.501014307073317, 'min_child_samples': 25}. Best is trial 21 with value: 0.8159660099828953.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trial LGBM AUC 0.8158985730677593 0.5414247609896364 0.9026042759496239\n",
            "filename ./models/lgbm_32 best_model_name./models/best_lgbm ; best_trial_number 21\n",
            "STARTING Params: {'metric': 'AUC', 'verbosity': 0, 'device': 'gpu', 'boosting_type': 'gbdt', 'subsample': 0.5872409883213553, 'learning_rate': 0.007405870921478667, 'num_iterations': 37569, 'subsample_freq': 1, 'lambda_l1': 2.8232682174433457, 'lambda_l2': 0.33103371803648324, 'num_leaves': 88, 'feature_fraction': 0.4537029051428528, 'min_child_samples': 25}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:574: UserWarning:\n",
            "\n",
            "Found `num_iterations` in params. Will use it instead of argument\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:578: UserWarning:\n",
            "\n",
            "'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "\n",
            "\u001b[32m[I 2021-09-17 21:17:04,391]\u001b[0m Trial 33 finished with value: 0.8159406290223051 and parameters: {'boosting_type': 'gbdt', 'subsample': 0.5872409883213553, 'eta': 0.007405870921478667, 'num_iterations': 37569, 'lambda_l1': 2.8232682174433457, 'lambda_l2': 0.33103371803648324, 'num_leaves': 88, 'feature_fraction': 0.4537029051428528, 'min_child_samples': 25}. Best is trial 21 with value: 0.8159660099828953.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trial LGBM AUC 0.8159406290223051 0.5418976699468102 0.9026679599241426\n",
            "filename ./models/lgbm_33 best_model_name./models/best_lgbm ; best_trial_number 21\n",
            "STARTING Params: {'metric': 'AUC', 'verbosity': 0, 'device': 'gpu', 'boosting_type': 'gbdt', 'subsample': 0.5820485724914063, 'learning_rate': 0.007493396037960933, 'num_iterations': 36778, 'subsample_freq': 1, 'lambda_l1': 0.5019768434336962, 'lambda_l2': 0.3365610761306776, 'num_leaves': 117, 'feature_fraction': 0.42592546355478395, 'min_child_samples': 18}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:574: UserWarning:\n",
            "\n",
            "Found `num_iterations` in params. Will use it instead of argument\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:578: UserWarning:\n",
            "\n",
            "'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "\n",
            "\u001b[32m[I 2021-09-17 21:41:52,996]\u001b[0m Trial 34 finished with value: 0.8154537413516078 and parameters: {'boosting_type': 'gbdt', 'subsample': 0.5820485724914063, 'eta': 0.007493396037960933, 'num_iterations': 36778, 'lambda_l1': 0.5019768434336962, 'lambda_l2': 0.3365610761306776, 'num_leaves': 117, 'feature_fraction': 0.42592546355478395, 'min_child_samples': 18}. Best is trial 21 with value: 0.8159660099828953.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trial LGBM AUC 0.8154537413516078 0.5412229927485859 0.9022232679558345\n",
            "filename ./models/lgbm_34 best_model_name./models/best_lgbm ; best_trial_number 21\n",
            "STARTING Params: {'metric': 'AUC', 'verbosity': 0, 'device': 'gpu', 'boosting_type': 'gbdt', 'subsample': 0.5786686492005797, 'learning_rate': 0.010206758549020305, 'num_iterations': 34755, 'subsample_freq': 1, 'lambda_l1': 2.506224854597924, 'lambda_l2': 0.07071316895661084, 'num_leaves': 39, 'feature_fraction': 0.4479920638729297, 'min_child_samples': 45}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:574: UserWarning:\n",
            "\n",
            "Found `num_iterations` in params. Will use it instead of argument\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:578: UserWarning:\n",
            "\n",
            "'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "\n",
            "\u001b[32m[I 2021-09-17 22:07:52,233]\u001b[0m Trial 35 finished with value: 0.8158328279049585 and parameters: {'boosting_type': 'gbdt', 'subsample': 0.5786686492005797, 'eta': 0.010206758549020305, 'num_iterations': 34755, 'lambda_l1': 2.506224854597924, 'lambda_l2': 0.07071316895661084, 'num_leaves': 39, 'feature_fraction': 0.4479920638729297, 'min_child_samples': 45}. Best is trial 21 with value: 0.8159660099828953.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trial LGBM AUC 0.8158328279049585 0.5410907404474383 0.9026977594756362\n",
            "filename ./models/lgbm_35 best_model_name./models/best_lgbm ; best_trial_number 21\n",
            "STARTING Params: {'metric': 'AUC', 'verbosity': 0, 'device': 'gpu', 'boosting_type': 'gbdt', 'subsample': 0.5858001913044534, 'learning_rate': 0.014963486413133473, 'num_iterations': 37831, 'subsample_freq': 1, 'lambda_l1': 0.3263372308411499, 'lambda_l2': 0.0027411645206884493, 'num_leaves': 86, 'feature_fraction': 0.549286209233687, 'min_child_samples': 58}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:574: UserWarning:\n",
            "\n",
            "Found `num_iterations` in params. Will use it instead of argument\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:578: UserWarning:\n",
            "\n",
            "'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "\n",
            "\u001b[32m[I 2021-09-17 22:18:40,653]\u001b[0m Trial 36 finished with value: 0.8148842012514935 and parameters: {'boosting_type': 'gbdt', 'subsample': 0.5858001913044534, 'eta': 0.014963486413133473, 'num_iterations': 37831, 'lambda_l1': 0.3263372308411499, 'lambda_l2': 0.0027411645206884493, 'num_leaves': 86, 'feature_fraction': 0.549286209233687, 'min_child_samples': 58}. Best is trial 21 with value: 0.8159660099828953.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trial LGBM AUC 0.8148842012514935 0.5423222215366673 0.9017611920561448\n",
            "filename ./models/lgbm_36 best_model_name./models/best_lgbm ; best_trial_number 21\n",
            "STARTING Params: {'metric': 'AUC', 'verbosity': 0, 'device': 'gpu', 'boosting_type': 'gbdt', 'subsample': 0.5737091401218981, 'learning_rate': 0.008405904035641693, 'num_iterations': 33334, 'subsample_freq': 1, 'lambda_l1': 0.023557698418466482, 'lambda_l2': 1.0423061646714944, 'num_leaves': 56, 'feature_fraction': 0.40266157500626626, 'min_child_samples': 29}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:574: UserWarning:\n",
            "\n",
            "Found `num_iterations` in params. Will use it instead of argument\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:578: UserWarning:\n",
            "\n",
            "'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "\n",
            "\u001b[32m[I 2021-09-17 22:39:27,796]\u001b[0m Trial 37 finished with value: 0.8155200899701329 and parameters: {'boosting_type': 'gbdt', 'subsample': 0.5737091401218981, 'eta': 0.008405904035641693, 'num_iterations': 33334, 'lambda_l1': 0.023557698418466482, 'lambda_l2': 1.0423061646714944, 'num_leaves': 56, 'feature_fraction': 0.40266157500626626, 'min_child_samples': 29}. Best is trial 21 with value: 0.8159660099828953.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trial LGBM AUC 0.8155200899701329 0.5419979310024404 0.9023354333005699\n",
            "filename ./models/lgbm_37 best_model_name./models/best_lgbm ; best_trial_number 21\n",
            "STARTING Params: {'metric': 'AUC', 'verbosity': 0, 'device': 'gpu', 'boosting_type': 'gbdt', 'subsample': 0.5908701351213503, 'learning_rate': 0.0058825298705675, 'num_iterations': 32418, 'subsample_freq': 1, 'lambda_l1': 0.056537876157297516, 'lambda_l2': 0.23817340025196476, 'num_leaves': 111, 'feature_fraction': 0.5903891528884171, 'min_child_samples': 20}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:574: UserWarning:\n",
            "\n",
            "Found `num_iterations` in params. Will use it instead of argument\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:578: UserWarning:\n",
            "\n",
            "'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "\n",
            "\u001b[32m[I 2021-09-17 23:09:19,200]\u001b[0m Trial 38 finished with value: 0.8154408944938037 and parameters: {'boosting_type': 'gbdt', 'subsample': 0.5908701351213503, 'eta': 0.0058825298705675, 'num_iterations': 32418, 'lambda_l1': 0.056537876157297516, 'lambda_l2': 0.23817340025196476, 'num_leaves': 111, 'feature_fraction': 0.5903891528884171, 'min_child_samples': 20}. Best is trial 21 with value: 0.8159660099828953.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trial LGBM AUC 0.8154408944938037 0.5423375850112588 0.902360917164229\n",
            "filename ./models/lgbm_38 best_model_name./models/best_lgbm ; best_trial_number 21\n",
            "STARTING Params: {'metric': 'AUC', 'verbosity': 0, 'device': 'gpu', 'boosting_type': 'gbdt', 'subsample': 0.5641972147140464, 'learning_rate': 0.007347787440390321, 'num_iterations': 37743, 'subsample_freq': 1, 'lambda_l1': 0.7809533651443774, 'lambda_l2': 0.04749261940282875, 'num_leaves': 21, 'feature_fraction': 0.4633328194247181, 'min_child_samples': 39}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:574: UserWarning:\n",
            "\n",
            "Found `num_iterations` in params. Will use it instead of argument\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:578: UserWarning:\n",
            "\n",
            "'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "\n",
            "\u001b[32m[I 2021-09-17 23:49:53,132]\u001b[0m Trial 39 finished with value: 0.8161146771285536 and parameters: {'boosting_type': 'gbdt', 'subsample': 0.5641972147140464, 'eta': 0.007347787440390321, 'num_iterations': 37743, 'lambda_l1': 0.7809533651443774, 'lambda_l2': 0.04749261940282875, 'num_leaves': 21, 'feature_fraction': 0.4633328194247181, 'min_child_samples': 39}. Best is trial 39 with value: 0.8161146771285536.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trial LGBM AUC 0.8161146771285536 0.5395740933702102 0.9028642837436492\n",
            "filename ./models/lgbm_39 best_model_name./models/best_lgbm ; best_trial_number 39\n",
            "STARTING Params: {'metric': 'AUC', 'verbosity': 0, 'device': 'gpu', 'boosting_type': 'gbdt', 'subsample': 0.5638177768312883, 'learning_rate': 0.009208784700868814, 'num_iterations': 24312, 'subsample_freq': 1, 'lambda_l1': 0.8849807552531815, 'lambda_l2': 0.007449926605562585, 'num_leaves': 16, 'feature_fraction': 0.8715899551870319, 'min_child_samples': 50}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:574: UserWarning:\n",
            "\n",
            "Found `num_iterations` in params. Will use it instead of argument\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:578: UserWarning:\n",
            "\n",
            "'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "\n",
            "\u001b[32m[I 2021-09-18 00:19:57,036]\u001b[0m Trial 40 finished with value: 0.8157762617451916 and parameters: {'boosting_type': 'gbdt', 'subsample': 0.5638177768312883, 'eta': 0.009208784700868814, 'num_iterations': 24312, 'lambda_l1': 0.8849807552531815, 'lambda_l2': 0.007449926605562585, 'num_leaves': 16, 'feature_fraction': 0.8715899551870319, 'min_child_samples': 50}. Best is trial 39 with value: 0.8161146771285536.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trial LGBM AUC 0.8157762617451916 0.5406528613328756 0.9027202518824334\n",
            "filename ./models/lgbm_40 best_model_name./models/best_lgbm ; best_trial_number 39\n",
            "STARTING Params: {'metric': 'AUC', 'verbosity': 0, 'device': 'gpu', 'boosting_type': 'gbdt', 'subsample': 0.573099085855387, 'learning_rate': 0.007244632851899241, 'num_iterations': 37447, 'subsample_freq': 1, 'lambda_l1': 3.5389432163435335, 'lambda_l2': 0.05401143320044411, 'num_leaves': 34, 'feature_fraction': 0.4646475688812316, 'min_child_samples': 40}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:574: UserWarning:\n",
            "\n",
            "Found `num_iterations` in params. Will use it instead of argument\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:578: UserWarning:\n",
            "\n",
            "'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "\n",
            "\u001b[32m[I 2021-09-18 00:45:51,696]\u001b[0m Trial 41 finished with value: 0.8159626259873438 and parameters: {'boosting_type': 'gbdt', 'subsample': 0.573099085855387, 'eta': 0.007244632851899241, 'num_iterations': 37447, 'lambda_l1': 3.5389432163435335, 'lambda_l2': 0.05401143320044411, 'num_leaves': 34, 'feature_fraction': 0.4646475688812316, 'min_child_samples': 40}. Best is trial 39 with value: 0.8161146771285536.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trial LGBM AUC 0.8159626259873438 0.5410004844425926 0.9025751429349581\n",
            "filename ./models/lgbm_41 best_model_name./models/best_lgbm ; best_trial_number 39\n",
            "STARTING Params: {'metric': 'AUC', 'verbosity': 0, 'device': 'gpu', 'boosting_type': 'gbdt', 'subsample': 0.572636020426708, 'learning_rate': 0.0074542698379182265, 'num_iterations': 37562, 'subsample_freq': 1, 'lambda_l1': 0.19494846317268774, 'lambda_l2': 0.06463389018961098, 'num_leaves': 29, 'feature_fraction': 0.4628614480374939, 'min_child_samples': 42}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:574: UserWarning:\n",
            "\n",
            "Found `num_iterations` in params. Will use it instead of argument\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:578: UserWarning:\n",
            "\n",
            "'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "\n",
            "\u001b[32m[I 2021-09-18 01:21:52,439]\u001b[0m Trial 42 finished with value: 0.8159337125393676 and parameters: {'boosting_type': 'gbdt', 'subsample': 0.572636020426708, 'eta': 0.0074542698379182265, 'num_iterations': 37562, 'lambda_l1': 0.19494846317268774, 'lambda_l2': 0.06463389018961098, 'num_leaves': 29, 'feature_fraction': 0.4628614480374939, 'min_child_samples': 42}. Best is trial 39 with value: 0.8161146771285536.\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trial LGBM AUC 0.8159337125393676 0.5403548117836003 0.9027852386674697\n",
            "filename ./models/lgbm_42 best_model_name./models/best_lgbm ; best_trial_number 39\n",
            "STARTING Params: {'metric': 'AUC', 'verbosity': 0, 'device': 'gpu', 'boosting_type': 'gbdt', 'subsample': 0.5724045336241672, 'learning_rate': 0.00751375927856499, 'num_iterations': 37487, 'subsample_freq': 1, 'lambda_l1': 0.1203929410045197, 'lambda_l2': 0.03988094618438434, 'num_leaves': 34, 'feature_fraction': 0.46206876729543683, 'min_child_samples': 61}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:574: UserWarning:\n",
            "\n",
            "Found `num_iterations` in params. Will use it instead of argument\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:578: UserWarning:\n",
            "\n",
            "'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cROM4PIENDIY"
      },
      "source": [
        "Trial 39 finished with value: 0.8161146771285536 and parameters: {'boosting_type': 'gbdt', 'subsample': 0.5641972147140464, 'eta': 0.007347787440390321, 'num_iterations': 37743, 'lambda_l1': 0.7809533651443774, 'lambda_l2': 0.04749261940282875, 'num_leaves': 21, 'feature_fraction': 0.4633328194247181, 'min_child_samples': 39}. Best is trial 39 with value: 0.8161146771285536.\n",
        "\n",
        "Trial LGBM AUC 0.8161146771285536 0.5395740933702102 0.9028642837436492\n",
        "\n",
        "Kaggle public score:  0.81790"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "aiEsLmfaGY9t",
        "outputId": "4d1cae8d-1680-42ed-89f9-6bc9af7d1f3e"
      },
      "source": [
        " # To get the dictionary of parameter name and parameter values:\n",
        " # Right now logging is not working as expected so this is pending\n",
        "#logging.info(f'Return a dictionary of parameter name and parameter values:{lgbm_study.best_params}')\n",
        "\n",
        "results = []\n",
        "print(f'Return a dictionary of parameter name and parameter values:{lgbm_study.best_params}')\n",
        "results.append({'msg':lgbm_study.best_params})\n",
        " \n",
        "# To get the best observed value of the objective function:\n",
        "#logging.info(f'Return the best observed value of the objective function: {lgbm_study.best_value}')\n",
        "print(f'Return the best observed value of the objective function: {lgbm_study.best_value}')\n",
        "results.append({'msg':lgbm_study.best_value})\n",
        " \n",
        "# To get the best trial:\n",
        "print(f'Return the best trial: {lgbm_study.best_trial}')\n",
        "results.append({'msg': lgbm_study.best_trial})\n",
        "\n",
        "pd.DataFrame(results).to_csv('results.csv', index=False)\n",
        " \n",
        "# To get all trials:\n",
        "#print(\"Return all the trials:\", study.trials)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-e22bb7b80d24>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Return a dictionary of parameter name and parameter values:{lgbm_study.best_params}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'msg'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlgbm_study\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'lgbm_study' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPpQ4qviZNcL"
      },
      "source": [
        "model_file = open('./models/best_lgbm', 'rb')\n",
        "colab_lgbm_model_eval = pickle.load(model_file)\n",
        "model_file.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2AQVqfF0ZTCg",
        "outputId": "2a177282-e3bf-486d-bd4a-4a3d9e3873b7"
      },
      "source": [
        "colab_lgbm_model_eval.fit(X,y)\n",
        "y_predproba = colab_lgbm_model_eval.predict_proba(X)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_label.py:235: DataConversionWarning:\n",
            "\n",
            "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_label.py:268: DataConversionWarning:\n",
            "\n",
            "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:177: UserWarning:\n",
            "\n",
            "Found `num_iterations` in params. Will use it instead of argument\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] feature_fraction is set=0.4633328194247181, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4633328194247181\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.04749261940282875, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.04749261940282875\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.7809533651443774, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7809533651443774\n",
            "[LightGBM] [Warning] feature_fraction is set=0.4633328194247181, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4633328194247181\n",
            "[LightGBM] [Warning] lambda_l2 is set=0.04749261940282875, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.04749261940282875\n",
            "[LightGBM] [Warning] lambda_l1 is set=0.7809533651443774, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.7809533651443774\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CKHGmPj9Fmli",
        "outputId": "658c852e-33f5-45dd-a698-a38e690432d9"
      },
      "source": [
        "roc_auc_score(y, y_predproba[:, 1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9017065674757444"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6sOlt5KmZgI2"
      },
      "source": [
        "y_predproba = pd.DataFrame(colab_lgbm_model_eval.predict_proba(test_X))\n",
        "y_predproba['id'] = y_predproba.index + 957919\n",
        "y_predproba.to_csv('./artifacts/lightgbm_predproba.csv', index=False)\n",
        "results_df = y_predproba.rename(columns={1:'claim'}).copy()\n",
        "results_df = results_df[['id','claim']]\n",
        "results_df.to_csv('./artifacts/lightgbm_submission.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83lgmBIhegiv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}